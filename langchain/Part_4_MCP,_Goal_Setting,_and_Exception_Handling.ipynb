{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPis0yaMPe/LtQpRguPiynB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-jamwal/blog-agentic-architectures/blob/main/Part_4_MCP%2C_Goal_Setting%2C_and_Exception_Handling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Agents in Python and n8n in 2026\n",
        "Companion to https://rajajamwal.substack.com/p/building-agents-in-python-and-n8n\n",
        "\n",
        "Subscribe to my blog, https://rajajamwal.substack.com"
      ],
      "metadata": {
        "id": "31p3WWR2HIsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MCP, Goal Setting, and Exception Handling\n",
        "\n",
        "Welcome to Part 4 of the **Agentic Design Patterns** series.\n",
        "\n",
        "We have built agents that can think (Part 2) and work in teams (Part 3). Now, we need to ensure they are **robust** and **interoperable**. A hobbyist agent works 80% of the time; a production agent must handle the other 20%â€”the errors, the timeouts, and the complex integrations.\n",
        "\n",
        "We will cover:\n",
        "1.  **Model Context Protocol (MCP):** The new standard for connecting AI to systems.\n",
        "2.  **Goal Setting & Monitoring:** Keeping the agent focused on the objective.\n",
        "3.  **Exception Handling:** Building \"safety nets\" for when things break.\n",
        "\n",
        "### The Stack\n",
        "*   **Python**\n",
        "*   **LangChain**\n",
        "*   **OpenAI** (GPT-4o-mini)\n",
        "\n",
        "### The n8n Connection\n",
        "*   **MCP** = Using the **HTTP Request Node** to query MCP servers, or using dedicated MCP nodes (if available in your version).\n",
        "*   **Goal Setting** = Using a **Set Node** to define a `goal` variable, and passing it into a loop (using the **Split In Batches** or **Loop** nodes) until a condition is met.\n",
        "*   **Exception Handling** = The **Error Trigger** node in n8n. If a node fails, the workflow jumps to a specific error-handling path (e.g., sending an alert or trying a different model)."
      ],
      "metadata": {
        "id": "wLSCAy6AHPy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Dependencies\n",
        "!pip install -qU langchain langchain-openai langchain-core\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# @title 2. Setup API Key\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Initialize the Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "print(\"âœ… Environment Setup Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko6aQfaiHh1u",
        "outputId": "9aeebd8d-b87d-4939-b2b0-c615043f0e47"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/84.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/489.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ… Environment Setup Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 10: Model Context Protocol (MCP)\n",
        "\n",
        "**The Problem:** Every time you want to connect an LLM to a new database (Google Drive, Slack, Postgres), you have to write custom integration code. It's a mess of proprietary APIs.\n",
        "\n",
        "**The Solution:** **MCP**. Think of it like \"USB for AI.\" It's a standard way to expose data (Resources) and actions (Tools) so *any* agent can use them without custom code.\n",
        "\n",
        "**The Scenario:** While we can't run a full MCP server inside Colab, we will simulate how an **MCP Client** (your agent) dynamically discovers and binds tools."
      ],
      "metadata": {
        "id": "m4leWHwgHy4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# --- Simulation: The \"MCP Server\" ---\n",
        "# Imagine this function lives on a remote server and is exposed via JSON-RPC.\n",
        "# The MCP standard defines how the agent \"discovers\" this tool.\n",
        "\n",
        "@tool\n",
        "def mcp_filesystem_read(filepath: str) -> str:\n",
        "    \"\"\"MCP Tool: Reads a file from the secure sandbox.\"\"\"\n",
        "    # In a real scenario, this accesses the actual disk.\n",
        "    mock_fs = {\n",
        "        \"config.json\": '{\"status\": \"active\", \"version\": \"1.0\"}',\n",
        "        \"notes.txt\": \"Meeting at 2 PM.\"\n",
        "    }\n",
        "    return mock_fs.get(filepath, \"Error: File not found.\")\n",
        "\n",
        "# --- The Client (Your Agent) ---\n",
        "# The agent doesn't need to know HOW the tool works, just that it exists.\n",
        "tools = [mcp_filesystem_read]\n",
        "\n",
        "# Bind the discovered tools to the LLM\n",
        "agent_with_mcp = llm.bind_tools(tools)\n",
        "\n",
        "# --- Execution ---\n",
        "query = \"Read the configuration file 'config.json' and tell me the version.\"\n",
        "response = agent_with_mcp.invoke(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Tool Call Generated: {response.tool_calls}\")\n",
        "\n",
        "# Note: In a real MCP setup, you would use an MCP Client library to\n",
        "# automatically fetch these tools from a URL (e.g., `sse://localhost:8000`)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWsA1uYSLcHD",
        "outputId": "1f4ce1bf-be9f-4903-feb0-131515d2e3ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Read the configuration file 'config.json' and tell me the version.\n",
            "Tool Call Generated: [{'name': 'mcp_filesystem_read', 'args': {'filepath': 'config.json'}, 'id': 'call_0ZIf46jhnQDKqMaG5fmjAGl1', 'type': 'tool_call'}]\n",
            "Query: Read the configuration file 'config.json' and tell me the version.\n",
            "Tool Call Generated: [{'name': 'mcp_filesystem_read', 'args': {'filepath': 'config.json'}, 'id': 'call_ZtyblAcaHSW6Z0Pn7CfSSREm', 'type': 'tool_call'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 11: Goal Setting and Monitoring (The Manager)\n",
        "\n",
        "**The Problem:** Agents often get distracted. They might generate code that runs but doesn't actually solve the user's specific problem.\n",
        "\n",
        "**The Solution:** **The Loop**. We set a clear Goal. The agent acts. Then, a \"Monitor\" (or Judge) checks if the goal was met. If not, the agent tries again.\n",
        "\n",
        "**The Scenario:** We want the agent to write a specific Python function. We will loop until the code passes a check."
      ],
      "metadata": {
        "id": "xiAfUnNAMEXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- 1. The Worker (Doer) ---\n",
        "worker_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a Python function to achieve this goal: {goal}. Return ONLY code.\"\n",
        ")\n",
        "worker_chain = worker_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- 2. The Monitor (Judge) ---\n",
        "monitor_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Goal: {goal}\n",
        "    Code Generated: {code}\n",
        "\n",
        "    Does this code accurately achieve the goal?\n",
        "    Respond exactly with 'YES' or 'NO'.\"\"\"\n",
        ")\n",
        "monitor_chain = monitor_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- 3. The Loop ---\n",
        "goal = \"Calculate the Fibonacci sequence up to N\"\n",
        "max_retries = 3\n",
        "attempt = 0\n",
        "success = False\n",
        "\n",
        "print(f\"ðŸŽ¯ Goal: {goal}\\n\")\n",
        "\n",
        "while attempt < max_retries and not success:\n",
        "    attempt += 1\n",
        "    print(f\"--- Attempt {attempt} ---\")\n",
        "\n",
        "    # Generate\n",
        "    code = worker_chain.invoke({\"goal\": goal})\n",
        "    print(f\"Generated Code Length: {len(code)} chars\")\n",
        "\n",
        "    # Monitor\n",
        "    verdict = monitor_chain.invoke({\"goal\": goal, \"code\": code})\n",
        "    print(f\"Monitor Verdict: {verdict}\")\n",
        "\n",
        "    if \"YES\" in verdict.upper():\n",
        "        success = True\n",
        "        print(\"\\nâœ… Goal Achieved!\")\n",
        "        print(code)\n",
        "    else:\n",
        "        print(\"âŒ Retrying...\")\n",
        "\n",
        "if not success:\n",
        "    print(\"\\nðŸ’€ Failed to achieve goal after max retries.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsIMABdKLrvC",
        "outputId": "f1324467-2175-4128-ac77-a4618393195e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ Goal: Calculate the Fibonacci sequence up to N\n",
            "\n",
            "--- Attempt 1 ---\n",
            "Generated Code Length: 174 chars\n",
            "Monitor Verdict: YES\n",
            "\n",
            "âœ… Goal Achieved!\n",
            "```python\n",
            "def fibonacci_up_to_n(n):\n",
            "    fib_sequence = []\n",
            "    a, b = 0, 1\n",
            "    while a <= n:\n",
            "        fib_sequence.append(a)\n",
            "        a, b = b, a + b\n",
            "    return fib_sequence\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 12: Exception Handling (The Safety Net)\n",
        "\n",
        "**The Problem:** APIs go down. Models get rate-limited. Inputs are malformed. If your script crashes, your user is unhappy.\n",
        "\n",
        "**The Solution:** **Fallbacks**. If the primary model fails, automatically switch to a backup model or a simpler logic path.\n",
        "\n",
        "**The Scenario:** We will simulate a broken model (using a fake model name) and demonstrate how LangChain automatically falls back to a working model."
      ],
      "metadata": {
        "id": "8NdrhuqaMeHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# --- 1. Define a Broken Model ---\n",
        "# This model name does not exist, so it will raise an error.\n",
        "broken_llm = ChatOpenAI(model=\"gpt-fake-model-v99\")\n",
        "\n",
        "# --- 2. Define the Backup Model ---\n",
        "# This is our reliable worker.\n",
        "backup_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# --- 3. Create the Fallback Chain ---\n",
        "# We try the broken one first. If it fails, we use the backup.\n",
        "# .with_fallbacks() is a powerful LangChain feature.\n",
        "resilient_llm = broken_llm.with_fallbacks([backup_llm])\n",
        "\n",
        "# --- Execution ---\n",
        "try:\n",
        "    print(\"Attempting to invoke chain...\")\n",
        "    # This will internally try 'gpt-fake-model', fail, catch the error,\n",
        "    # and seamlessly switch to 'gpt-4o-mini'.\n",
        "    response = resilient_llm.invoke(\"Say 'Hello, I am the backup!'\")\n",
        "    print(f\"\\nSUCCESS: {response.content}\")\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL FAILURE: {e}\")\n",
        "\n",
        "# Note: This is crucial for production. You can fallback from GPT-4 (expensive)\n",
        "# to GPT-3.5 (cheap) or from OpenAI to Anthropic (different provider)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pRQ4khBMfgi",
        "outputId": "fea12e28-3d44-4cdb-ea6d-d45d3d8804f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to invoke chain...\n",
            "\n",
            "SUCCESS: Hello, I am the backup!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "You have now built the infrastructure for a professional agent:\n",
        "\n",
        "1.  **MCP:** You understand how to standardize tool connections.\n",
        "2.  **Goal Setting:** You created a loop that ensures quality before finishing.\n",
        "3.  **Exception Handling:** You built a system that doesn't crash when errors occur.\n",
        "\n",
        "In **Part 5**, we will focus on **Reliability**. We will bring humans into the loop and connect our agents to vast knowledge bases using **RAG**."
      ],
      "metadata": {
        "id": "AhaAf1n_My3y"
      }
    }
  ]
}