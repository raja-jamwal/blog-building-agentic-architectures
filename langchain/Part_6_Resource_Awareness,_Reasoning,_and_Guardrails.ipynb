{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-jamwal/blog-building-agentic-architectures/blob/main/langchain/Part_6_Resource_Awareness%2C_Reasoning%2C_and_Guardrails.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OITzObw3fqcX"
      },
      "source": [
        "# Building Agents in Python and n8n in 2026\n",
        "Companion to https://rajajamwal.substack.com/p/building-agents-in-python-and-n8n\n",
        "\n",
        "Subscribe to my blog, https://rajajamwal.substack.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnZ-M1FJf6TO"
      },
      "source": [
        "We have built agents that are smart, collaborative, and connected. Now, we need to make them **efficient** and **safe**.\n",
        "\n",
        "In production, you cannot afford to run every \"Hello World\" query through the most expensive model (GPT-4o/Claude 3.5 Sonnet). Nor can you afford to let an agent output unstructured garbage that breaks your frontend.\n",
        "\n",
        "We will cover:\n",
        "1.  **Resource-Aware Optimization:** Dynamically routing tasks to the cheapest model that can handle them.\n",
        "2.  **Reasoning Techniques:** Forcing the model to \"think\" (Chain of Thought) to solve hard problems.\n",
        "3.  **Guardrails:** Enforcing strict output structures and safety checks.\n",
        "\n",
        "### The Stack\n",
        "*   **Python**\n",
        "*   **LangChain**\n",
        "*   **OpenAI** (GPT-4o vs GPT-4o-mini)\n",
        "*   **Pydantic** (For data validation)\n",
        "\n",
        "### The n8n Connection\n",
        "*   **Resource-Awareness** = A **Switch Node** that checks the complexity of the input. Path A goes to a \"Basic LLM\" node (Mini), Path B goes to an \"Advanced LLM\" node.\n",
        "*   **Reasoning** = Prompt Engineering within the AI Agent node. Adding \"Let's think step by step\" to the system prompt.\n",
        "*   **Guardrails** = The **Structured Output Parser** in n8n (often found in the Advanced AI nodes) or using an **If Node** to check if the output contains forbidden words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdJWAFg0gLwo",
        "outputId": "140106be-4aad-4063-9355-36b2bb92364b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m463.6/463.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.50.0 requires pydantic<=2.12.3,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mEnter your OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ… Environment Setup Complete.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies\n",
        "!pip install -qU langchain langchain-openai langchain-core pydantic\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# @title 2. Setup API Key\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Initialize Models\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# We need two distinct models for the optimization pattern\n",
        "cheap_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "expensive_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "print(\"âœ… Environment Setup Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pix98UDUgdwa"
      },
      "source": [
        "## Pattern 16: Resource-Aware Optimization\n",
        "\n",
        "**The Problem:** Using a flagship model (like GPT-4o) for simple tasks (like \"What is the capital of France?\") is a waste of money and adds latency.\n",
        "\n",
        "**The Solution:** **Dynamic Routing**. We use a tiny, fast model to classify the *complexity* of the request. Based on that classification, we route the work to the appropriate model.\n",
        "\n",
        "**The Scenario:**\n",
        "*   **Simple:** \"What is 2+2?\" -> Route to `gpt-4o-mini`.\n",
        "*   **Complex:** \"Write a Python script to scrape a website and analyze sentiment.\" -> Route to `gpt-4o`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jus9Fm0Sgjsz",
        "outputId": "849aaceb-7186-499d-8833-5063f8c74066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Test 1: Simple Query ---\n",
            "Q: Who is the president of the USA?\n",
            "A: As of October 2023, the president of the USA is Joe Biden.\n",
            "\n",
            "--- Test 2: Complex Query ---\n",
            "Q: Write a Python class for a Binary Search Tree with insert and delete methods.\n",
            "A: Creating a Binary Search Tree (BST) in Python involves defining a class for the tree itself and a cl...\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableBranch, RunnablePassthrough\n",
        "\n",
        "# --- 1. The Classifier (The Traffic Cop) ---\n",
        "# We use the cheap model to decide where to send the request.\n",
        "classifier_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Given the user request below, classify it as either 'SIMPLE' or 'COMPLEX'.\n",
        "    - Simple: Factual questions, greetings, basic summaries.\n",
        "    - Complex: Coding, creative writing, reasoning, multi-step logic.\n",
        "\n",
        "    Return ONLY the word 'SIMPLE' or 'COMPLEX'.\n",
        "\n",
        "    Request: {question}\"\"\"\n",
        ")\n",
        "classifier_chain = classifier_prompt | cheap_llm | StrOutputParser()\n",
        "\n",
        "# --- 2. The Branches ---\n",
        "chain_simple = (\n",
        "    ChatPromptTemplate.from_template(\"You are a fast assistant. Answer briefly: {question}\")\n",
        "    | cheap_llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain_complex = (\n",
        "    ChatPromptTemplate.from_template(\"You are an expert reasoner. Provide a detailed answer: {question}\")\n",
        "    | expensive_llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- 3. The Routing Logic ---\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"SIMPLE\" in x[\"topic\"].upper(), chain_simple),\n",
        "    chain_complex # Default fallback (usually the stronger model)\n",
        ")\n",
        "\n",
        "# --- 4. The Full Chain ---\n",
        "full_chain = (\n",
        "    {\"topic\": classifier_chain, \"question\": RunnablePassthrough()}\n",
        "    | branch\n",
        ")\n",
        "\n",
        "# --- Execution ---\n",
        "print(\"--- Test 1: Simple Query ---\")\n",
        "q1 = \"Who is the president of the USA?\"\n",
        "print(f\"Q: {q1}\")\n",
        "# This should route to Mini (Fast)\n",
        "print(f\"A: {full_chain.invoke(q1)}\\n\")\n",
        "\n",
        "print(\"--- Test 2: Complex Query ---\")\n",
        "q2 = \"Write a Python class for a Binary Search Tree with insert and delete methods.\"\n",
        "print(f\"Q: {q2}\")\n",
        "# This should route to GPT-4o (Slow but smart)\n",
        "# Note: We won't print the whole code to save space, just the start.\n",
        "result = full_chain.invoke(q2)\n",
        "print(f\"A: {result[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKH2FVMVhXee"
      },
      "source": [
        "## Pattern 17: Reasoning Techniques (Chain of Thought)\n",
        "\n",
        "**The Problem:** If you ask an LLM a trick question, it often guesses the answer immediately based on probability, which leads to errors.\n",
        "\n",
        "**The Solution:** **Chain of Thought (CoT)**. We explicitly instruct the model to \"think step-by-step\" before answering. This forces the model to generate intermediate tokens, which act as a \"scratchpad\" for its logic.\n",
        "\n",
        "**The Scenario:** A classic logic puzzle that models often get wrong without reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ-Prb-vhYj7",
        "outputId": "a458a39f-a897-4222-ad79-9a3701de9201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§© Puzzle: If I hang 5 shirts to dry and they take 5 hours to dry, how long will it take to dry 10 shirts?\n",
            "\n",
            "--- âŒ Direct Answer (Risk of Error) ---\n",
            "If you hang 10 shirts to dry at the same time as the 5 shirts, they will still take 5 hours to dry, assuming there is enough space and airflow for all the shirts to dry simultaneously. So, it will take 5 hours to dry 10 shirts.\n",
            "\n",
            "--- âœ… Chain of Thought (Robust) ---\n",
            "1. **Identify the variables:**\n",
            "   - Let \\( S \\) be the number of shirts.\n",
            "   - Let \\( T \\) be the time taken to dry the shirts.\n",
            "   - From the question, we know that \\( S = 5 \\) shirts take \\( T = 5 \\) hours to dry.\n",
            "\n",
            "2. **Analyze the constraints:**\n",
            "   - The drying process is assumed to be independent of the number of shirts, meaning that drying 5 shirts takes the same amount of time as drying 10 shirts, provided there is enough space to hang all the shirts simultaneously.\n",
            "   - If the drying space is limited and cannot accommodate all shirts at once, the drying time would increase. However, the question does not specify any limitations on drying space.\n",
            "\n",
            "3. **Formulate the logic:**\n",
            "   - Since the drying time is independent of the number of shirts (assuming they can all be hung at the same time), the time taken to dry 10 shirts will be the same as the time taken to dry 5 shirts.\n",
            "   - Therefore, if 5 shirts take 5 hours, then 10 shirts will also take 5 hours, as long as they can dry simultaneously.\n",
            "\n",
            "4. **Give the final answer:**\n",
            "   - It will take **5 hours** to dry 10 shirts.\n"
          ]
        }
      ],
      "source": [
        "# --- The Puzzle ---\n",
        "puzzle = \"If I hang 5 shirts to dry and they take 5 hours to dry, how long will it take to dry 10 shirts?\"\n",
        "\n",
        "# --- Approach 1: Zero-Shot (Direct Answer) ---\n",
        "# Often fails or gives a quick, wrong answer.\n",
        "direct_prompt = ChatPromptTemplate.from_template(\"Answer this: {puzzle}\")\n",
        "direct_chain = direct_prompt | cheap_llm | StrOutputParser()\n",
        "\n",
        "# --- Approach 2: Chain of Thought (Reasoning) ---\n",
        "# Forces the model to break it down.\n",
        "cot_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Answer the following question.\n",
        "    CRITICAL: You must think step-by-step.\n",
        "    1. Identify the variables.\n",
        "    2. Analyze the constraints.\n",
        "    3. Formulate the logic.\n",
        "    4. Give the final answer.\n",
        "\n",
        "    Question: {puzzle}\"\"\"\n",
        ")\n",
        "cot_chain = cot_prompt | cheap_llm | StrOutputParser()\n",
        "\n",
        "# --- Execution ---\n",
        "print(f\"ğŸ§© Puzzle: {puzzle}\\n\")\n",
        "\n",
        "print(\"--- âŒ Direct Answer (Risk of Error) ---\")\n",
        "print(direct_chain.invoke({\"puzzle\": puzzle}))\n",
        "\n",
        "print(\"\\n--- âœ… Chain of Thought (Robust) ---\")\n",
        "print(cot_chain.invoke({\"puzzle\": puzzle}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7L_teIPhtFa"
      },
      "source": [
        "## Pattern 18: Guardrails (Safety & Structure)\n",
        "\n",
        "**The Problem:** You are building an API that expects JSON output. If the LLM adds \"Here is your JSON:\" before the actual JSON, your code breaks. Or worse, the LLM generates toxic content.\n",
        "\n",
        "**The Solution:** **Pydantic Parsers**. We define a strict data class (Schema). We pass this schema to the LLM. The LLM *must* return data that fits this shape, or the system throws a validation error.\n",
        "\n",
        "**The Scenario:** We want to extract sentiment from a review, but it MUST be strictly formatted as `{\"sentiment\": \"POS/NEG\", \"score\": 0-10}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8z8NgnYhjXx",
        "outputId": "308cded2-bb81-44eb-c01e-82017fbcbc44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ğŸ›¡ï¸ Executing Guardrailed Chain ---\n",
            "Type: <class '__main__.SentimentAnalysis'>\n",
            "Sentiment: NEGATIVE\n",
            "Score: 4\n",
            "Reasoning: The food was only okay, and the service was poor with a long wait time.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "\n",
        "# --- 1. Define the Guardrail (Schema) ---\n",
        "class SentimentAnalysis(BaseModel):\n",
        "    sentiment: str = Field(description=\"Must be either 'POSITIVE' or 'NEGATIVE'\")\n",
        "    score: int = Field(description=\"A score between 1 and 10\")\n",
        "    reasoning: str = Field(description=\"A brief explanation\")\n",
        "\n",
        "    @field_validator(\"score\")\n",
        "    @classmethod\n",
        "    def check_score_range(cls, value: int):\n",
        "        if not 1 <= value <= 10:\n",
        "            raise ValueError(\"Score must be between 1 and 10!\")\n",
        "        return value\n",
        "\n",
        "# --- 2. Setup the Parser ---\n",
        "parser = PydanticOutputParser(pydantic_object=SentimentAnalysis)\n",
        "\n",
        "# --- 3. The Prompt with Instructions ---\n",
        "# parser.get_format_instructions() automatically generates the prompt\n",
        "# telling the LLM how to format the JSON.\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Analyze the review below.\n",
        "    {format_instructions}\n",
        "\n",
        "    Review: {review}\"\"\"\n",
        ")\n",
        "\n",
        "# --- 4. The Chain ---\n",
        "# The parser is the final step. It takes the text and converts it to a Python Object.\n",
        "safe_chain = prompt | cheap_llm | parser\n",
        "\n",
        "# --- Execution ---\n",
        "review_text = \"The food was okay, but the service was terrible. I waited 40 minutes.\"\n",
        "\n",
        "try:\n",
        "    print(\"--- ğŸ›¡ï¸ Executing Guardrailed Chain ---\")\n",
        "    result = safe_chain.invoke({\n",
        "        \"review\": review_text,\n",
        "        \"format_instructions\": parser.get_format_instructions()\n",
        "    })\n",
        "\n",
        "    print(f\"Type: {type(result)}\") # It's a Pydantic object, not a string!\n",
        "    print(f\"Sentiment: {result.sentiment}\")\n",
        "    print(f\"Score: {result.score}\")\n",
        "    print(f\"Reasoning: {result.reasoning}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Guardrail Triggered! Validation Failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgr5nQgQj7wN"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You have now engineered a system that is:\n",
        "\n",
        "1.  **Cost-Effective:** It uses the right model for the right task.\n",
        "2.  **Smart:** It thinks before it speaks.\n",
        "3.  **Safe:** It guarantees structured, valid outputs.\n",
        "\n",
        "In **Part 7**, the final chapter, we will look at **Strategy**. We will build an **Evaluation Pipeline** (LLM-as-a-Judge) to automatically grade our agents."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPUv1ll63q1590KT2O4aJ+2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
