{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpUoFrolyaZQCHo7n70yOd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-jamwal/-medicine-notes-segmenter/blob/main/Part_1_The_Foundation_Prompt_Chaining%2C_Routing%2C_and_Parallelization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Agents in Python and n8n in 2026\n",
        "Companion to https://rajajamwal.substack.com/p/building-agents-in-python-and-n8n\n",
        "\n",
        "Subscribe to my blog, https://rajajamwal.substack.com"
      ],
      "metadata": {
        "id": "zY4pxJ6q8Nt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we get to complex autonomous agents, we must master the flow of data.\n",
        "\n",
        "We will cover:\n",
        "1.  **Prompt Chaining:** Breaking complex tasks into sequential steps.\n",
        "2.  **Routing:** Adding conditional logic to direct requests to the right expert.\n",
        "3.  **Parallelization:** Running independent tasks simultaneously for speed.\n",
        "\n",
        "### The Stack\n",
        "*   **Python**\n",
        "*   **LangChain (LCEL):** For orchestration.\n",
        "*   **OpenAI:** As our LLM provider.\n",
        "\n",
        "### The n8n Connection\n",
        "If you are following along with **n8n**:\n",
        "*   **Chaining** = Connecting nodes linearly.\n",
        "*   **Routing** = The `Switch` or `If` Node.\n",
        "*   **Parallelization** = Connecting one node to multiple outputs + `Merge` Node."
      ],
      "metadata": {
        "id": "WEm6WP8I8blJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Dependencies\n",
        "# We need langchain and the openai integration\n",
        "!pip install -qU langchain langchain-openai langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoAhQ4kk8t1U",
        "outputId": "93c76cf7-ab47-4e3e-8849-d05629277fde"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/489.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/489.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# @title 2. Setup API Key\n",
        "# Enter your OpenAI API Key when prompted\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Initialize the Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# We use a temperature of 0 for consistent, deterministic results\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "print(\"✅ Environment Setup Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3II0p-ay80kN",
        "outputId": "04491a30-1af9-4c12-982c-e8f305a4977f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenAI API Key: ··········\n",
            "✅ Environment Setup Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 1: Prompt Chaining (The Pipeline)\n",
        "\n",
        "**The Problem:** Asking an LLM to do too much in one prompt (e.g., \"Research this company, write a summary, and translate it to Spanish\") often leads to hallucinations or missed instructions.\n",
        "\n",
        "**The Solution:** Break the task into a sequence. The output of Step 1 becomes the input of Step 2.\n",
        "\n",
        "**The Scenario:** We want to generate a **Company Name** based on a product description, and then write a **Slogan** for that specific name."
      ],
      "metadata": {
        "id": "jEuLrcCS9TVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- Step 1: Generate Company Name ---\n",
        "name_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is a catchy, futuristic company name for a startup that makes: {product}? Return ONLY the name.\"\n",
        ")\n",
        "# Chain 1: Input -> Prompt -> LLM -> String Output\n",
        "name_chain = name_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- Step 2: Generate Slogan ---\n",
        "slogan_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 3-word slogan for a company named '{company_name}'.\"\n",
        ")\n",
        "slogan_chain = slogan_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- The Full Chain (LCEL) ---\n",
        "# We pipe the output of name_chain into the 'company_name' variable for the slogan_chain\n",
        "overall_chain = (\n",
        "    {\"company_name\": name_chain}\n",
        "    | slogan_chain\n",
        ")\n",
        "\n",
        "# --- Execution ---\n",
        "product_description = \"A coffee machine that uses AI to predict exactly what you want to drink\"\n",
        "result = overall_chain.invoke({\"product\": product_description})\n",
        "\n",
        "print(f\"Input Product: {product_description}\")\n",
        "print(f\"Final Result (Slogan): {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvhcXSMI9gv2",
        "outputId": "ccb21db4-b196-43fd-9114-c64bdb63d6d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Product: A coffee machine that uses AI to predict exactly what you want to drink\n",
            "Final Result (Slogan): \"Crafting Coffee Innovation.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 2: Routing (The Decision Maker)\n",
        "\n",
        "**The Problem:** A single prompt cannot handle every type of user request efficiently. You don't want a \"Creative Writer\" model handling \"Math\" questions.\n",
        "\n",
        "**The Solution:** Use a **Router** step to classify the intent, then direct the flow to a specialized chain.\n",
        "\n",
        "**The Scenario:** We are building a support bot.\n",
        "*   If the user asks about **Math/Logic**, route to a \"Logic Chain\".\n",
        "*   If the user asks for **Creative Writing**, route to a \"Creative Chain\"."
      ],
      "metadata": {
        "id": "tVJJ97pW9xpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableBranch, RunnablePassthrough\n",
        "\n",
        "# --- Define the Specialized Chains ---\n",
        "\n",
        "# 1. Math Chain (Strict, concise)\n",
        "math_chain = (\n",
        "    ChatPromptTemplate.from_template(\"You are a mathematician. Solve this: {input}\")\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 2. Creative Chain (Flowery, descriptive)\n",
        "creative_chain = (\n",
        "    ChatPromptTemplate.from_template(\"You are a poet. Write about: {input}\")\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- The Router ---\n",
        "# First, we ask the LLM to classify the input\n",
        "classification_chain = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"Classify the following input as either 'MATH' or 'CREATIVE'. Return ONLY the word.\\nInput: {input}\"\n",
        "    )\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Define the Branching Logic\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"MATH\" in x[\"topic\"], math_chain),      # If topic is MATH, go to math_chain\n",
        "    (lambda x: \"CREATIVE\" in x[\"topic\"], creative_chain), # If topic is CREATIVE, go to creative_chain\n",
        "    creative_chain # Default fallback\n",
        ")\n",
        "\n",
        "# --- The Full Routing Chain ---\n",
        "full_chain = (\n",
        "    {\"topic\": classification_chain, \"input\": RunnablePassthrough()}\n",
        "    | branch\n",
        ")\n",
        "\n",
        "# --- Execution ---\n",
        "print(\"--- Test 1: Math ---\")\n",
        "print(full_chain.invoke({\"input\": \"What is the square root of 144?\"}))\n",
        "\n",
        "print(\"\\n--- Test 2: Creative ---\")\n",
        "print(full_chain.invoke({\"input\": \"The sunset over the ocean\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96Bq9wg93Se",
        "outputId": "baf9d70e-4b94-42fb-c984-5c633e80b3f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test 1: Math ---\n",
            "The square root of 144 is 12.\n",
            "\n",
            "--- Test 2: Creative ---\n",
            "In the hush of evening's gentle sigh,  \n",
            "The sun descends, a fiery eye,  \n",
            "Casting gold upon the waves,  \n",
            "Where whispers of the ocean braves.  \n",
            "\n",
            "A canvas stretched, the sky ablaze,  \n",
            "With hues of crimson, orange, and mauve,  \n",
            "Each brushstroke dances, a fleeting phase,  \n",
            "As day concedes to night’s soft grove.  \n",
            "\n",
            "The horizon swallows the molten sphere,  \n",
            "A liquid ember, drawing near,  \n",
            "While waves, like secrets, lap the shore,  \n",
            "In rhythmic tales of ancient lore.  \n",
            "\n",
            "The salty breeze, a lover's breath,  \n",
            "Caresses skin, a sweet caress,  \n",
            "As shadows stretch and daylight wanes,  \n",
            "The world transforms, yet still remains.  \n",
            "\n",
            "In this moment, time stands still,  \n",
            "The heart is full, the spirit thrills,  \n",
            "For in the sunset’s warm embrace,  \n",
            "We find our peace, our sacred space.  \n",
            "\n",
            "So let us linger, hand in hand,  \n",
            "As twilight weaves its magic strand,  \n",
            "For in the sunset over the sea,  \n",
            "We glimpse eternity, wild and free.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 3: Parallelization (The Efficiency Booster)\n",
        "\n",
        "**The Problem:** Sometimes a task requires multiple independent perspectives (e.g., \"Pros\" and \"Cons\"). Running them one after another (sequentially) is slow.\n",
        "\n",
        "**The Solution:** Run them at the same time (in parallel) and merge the results at the end.\n",
        "\n",
        "**The Scenario:** We are analyzing a product. We want to generate a list of **Pros** and a list of **Cons** simultaneously, then combine them into a final review."
      ],
      "metadata": {
        "id": "r4MWU29_-Or-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "# --- Define Independent Tasks ---\n",
        "\n",
        "# Task 1: Identify Pros\n",
        "pros_chain = (\n",
        "    ChatPromptTemplate.from_template(\"List 3 distinct PROS of: {product}\")\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Task 2: Identify Cons\n",
        "cons_chain = (\n",
        "    ChatPromptTemplate.from_template(\"List 3 distinct CONS of: {product}\")\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- The Parallel Map ---\n",
        "# RunnableParallel runs these keys at the same time\n",
        "map_chain = RunnableParallel(\n",
        "    pros=pros_chain,\n",
        "    cons=cons_chain\n",
        ")\n",
        "\n",
        "# --- The Final Synthesis ---\n",
        "# Combines the parallel outputs into one final summary\n",
        "synthesis_chain = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"Combine these into a balanced review:\\n\\nPROS:\\n{pros}\\n\\nCONS:\\n{cons}\"\n",
        "    )\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- Full Chain ---\n",
        "parallel_workflow = map_chain | synthesis_chain\n",
        "\n",
        "# --- Execution ---\n",
        "topic = \"Remote Work\"\n",
        "result = parallel_workflow.invoke({\"product\": topic})\n",
        "\n",
        "print(f\"Topic: {topic}\\n\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAYGLxBu-UYb",
        "outputId": "4857beeb-bf62-4bf5-a6b0-e62e9276e103"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: Remote Work\n",
            "\n",
            "**Balanced Review of Remote Work**\n",
            "\n",
            "Remote work has become increasingly popular, offering a mix of advantages and challenges that can significantly impact both employees and employers. \n",
            "\n",
            "**Pros:**\n",
            "\n",
            "One of the most notable benefits of remote work is the **flexibility and work-life balance** it provides. Employees can often set their own schedules, allowing them to manage personal responsibilities—such as childcare or errands—while still meeting professional obligations. This flexibility can lead to increased job satisfaction and overall well-being.\n",
            "\n",
            "Another significant advantage is the **reduction in commuting time and costs**. By eliminating daily travel, employees save both time and money, which can alleviate stress and provide more opportunities to focus on work or personal activities. This aspect of remote work can enhance productivity and contribute positively to an employee's quality of life.\n",
            "\n",
            "For employers, remote work expands the **access to a broader talent pool**. Companies can hire individuals from various geographic locations, leading to a more diverse workforce and the ability to find specialized skills that may not be available locally. This can foster innovation and bring fresh perspectives to the organization.\n",
            "\n",
            "**Cons:**\n",
            "\n",
            "However, remote work is not without its drawbacks. One significant concern is the potential for **isolation and loneliness**. Employees may miss the social interactions and camaraderie that come with a physical office environment, which can negatively impact mental well-being and team cohesion.\n",
            "\n",
            "Additionally, while remote work offers flexibility, it can also create **work-life balance challenges**. The lines between personal and professional life can blur, making it difficult for employees to \"switch off\" from work. This can lead to longer hours, increased stress, and a higher risk of burnout.\n",
            "\n",
            "Finally, **communication barriers** can pose challenges in a remote work setting. Without face-to-face interactions, misunderstandings may arise more easily, and reliance on digital communication tools can lead to information overload or missed messages. These issues can hinder effective teamwork and collaboration.\n",
            "\n",
            "In conclusion, while remote work offers significant benefits such as flexibility, reduced commuting, and access to a wider talent pool, it also presents challenges like isolation, blurred work-life boundaries, and communication difficulties. Striking a balance between these pros and cons is essential for maximizing the effectiveness of remote work arrangements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You have just implemented the three fundamental patterns of Agentic AI:\n",
        "\n",
        "1.  **Chaining:** `A -> B` (Sequential)\n",
        "2.  **Routing:** `If X -> A, Else -> B` (Conditional)\n",
        "3.  **Parallelization:** `A + B -> C` (Simultaneous)\n",
        "\n",
        "In **Part 2**, we will make our agents smarter by introducing **Reflection** (Self-Correction) and **Tool Use** (Connecting to the outside world)"
      ],
      "metadata": {
        "id": "a5Nz3gNS-rid"
      }
    }
  ]
}
