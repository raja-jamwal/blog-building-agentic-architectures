{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2zMPkz77fmXvLU1YCDC4c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-jamwal/blog-agentic-architectures/blob/main/Part_3_Multi_Agent_Collaboration%2C_Memory%2C_and_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Agents in Python and n8n in 2026\n",
        "Companion to https://rajajamwal.substack.com/p/building-agents-in-python-and-n8n\n",
        "\n",
        "Subscribe to my blog, https://rajajamwal.substack.com"
      ],
      "metadata": {
        "id": "Wo_VM_pjEw9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Agent Collaboration, Memory, and Learning\n",
        "\n",
        "Welcome to Part 3 of the **Agentic Design Patterns** series.\n",
        "\n",
        "In Parts 1 and 2, we built single agents that could reason and act. But in the real world, complex problems require **teams** and **context**. A doctor doesn't work without a nurse; a lawyer doesn't work without case files.\n",
        "\n",
        "Today, we transform our isolated agents into collaborative, stateful systems.\n",
        "\n",
        "We will cover:\n",
        "1.  **Multi-Agent Collaboration:** Orchestrating specialized agents (Researcher + Writer).\n",
        "2.  **Memory Management:** Enabling the agent to remember past interactions.\n",
        "3.  **Learning (Few-Shot):** Teaching the agent to adapt its style using examples.\n",
        "\n",
        "### The Stack\n",
        "*   **Python**\n",
        "*   **LangChain**\n",
        "*   **OpenAI** (GPT-4o-mini)\n",
        "\n",
        "### The n8n Connection\n",
        "*   **Multi-Agent** = Chaining multiple **AI Agent Nodes**. For example, Agent A's output becomes Agent B's input variable.\n",
        "*   **Memory** = Connecting a **Window Buffer Memory** node to your AI Agent. This automatically handles the context window.\n",
        "*   **Learning** = Using a **Vector Store** to retrieve \"successful past examples\" and injecting them into the prompt (Few-Shot RAG)."
      ],
      "metadata": {
        "id": "FfXrej7iE2b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Dependencies\n",
        "!pip install -qU langchain langchain-openai langchain-core\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# @title 2. Setup API Key\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Initialize the Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "print(\"‚úÖ Environment Setup Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYpahF4IFGCE",
        "outputId": "4e4223fb-4258-478d-c43c-2eadb3fcdd47"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/84.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/489.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m481.3/489.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Environment Setup Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 7: Multi-Agent Collaboration (The Team)\n",
        "\n",
        "**The Problem:** A \"Jack of all trades\" agent is often a master of none. If you ask one prompt to \"Research deep quantum physics and write a poem about it,\" it might hallucinate the physics or write a boring poem.\n",
        "\n",
        "**The Solution:** **Specialization**. Create one agent solely for Research (factual, dry) and another for Writing (creative, engaging). Pass the baton from one to the other.\n",
        "\n",
        "**The Scenario:** We want to write a LinkedIn post about a technical topic.\n",
        "1.  **Agent A (Researcher):** Extracts key facts.\n",
        "2.  **Agent B (Writer):** Turns facts into a viral post."
      ],
      "metadata": {
        "id": "nTL6CPgBFT5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- Agent 1: The Researcher ---\n",
        "# Role: Strict, factual, bullet points.\n",
        "researcher_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a Research Analyst.\n",
        "    Given the topic '{topic}', list 3 key technical facts.\n",
        "    Be concise and factual. No fluff.\"\"\"\n",
        ")\n",
        "researcher_agent = researcher_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- Agent 2: The Writer ---\n",
        "# Role: Engaging, uses emojis, professional but fun.\n",
        "writer_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a LinkedIn Ghostwriter.\n",
        "    Take the following facts and write a short, engaging post.\n",
        "    Use emojis and a call to action.\n",
        "\n",
        "    Facts:\n",
        "    {facts}\"\"\"\n",
        ")\n",
        "writer_agent = writer_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- Orchestration (The Handoff) ---\n",
        "# We chain them together: Input -> Researcher -> Writer -> Output\n",
        "chain = (\n",
        "    {\"facts\": researcher_agent}\n",
        "    | writer_agent\n",
        ")\n",
        "\n",
        "# --- Execution ---\n",
        "topic = \"The impact of AI on Junior Developers\"\n",
        "print(f\"Topic: {topic}\\n\")\n",
        "print(\"--- ü§ñ Agents Collaborating... ---\")\n",
        "result = chain.invoke({\"topic\": topic})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKtd1RjPFbjq",
        "outputId": "0bf15505-7e17-4c8e-b789-ca6722b38b01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: The impact of AI on Junior Developers\n",
            "\n",
            "--- ü§ñ Agents Collaborating... ---\n",
            "üöÄ **Unlocking Potential with AI in Development!** üíª‚ú®\n",
            "\n",
            "Hey, tech enthusiasts! üåü Did you know that AI is revolutionizing the way junior developers work? Here are three game-changing ways AI is making a difference:\n",
            "\n",
            "1Ô∏è‚É£ **Code Generation Tools**: With tools like GitHub Copilot and OpenAI Codex, junior developers can receive real-time code suggestions. This means less time on boilerplate code and more time tackling complex problems! üõ†Ô∏è\n",
            "\n",
            "2Ô∏è‚É£ **Automated Testing & Debugging**: AI is stepping up the game in testing frameworks by predicting bugs and suggesting fixes. This not only enhances code quality but also streamlines the debugging process! üêûüîç\n",
            "\n",
            "3Ô∏è‚É£ **Skill Development**: AI-driven platforms are personalizing learning experiences, helping developers quickly master new languages and frameworks. Adaptive learning paths mean you can focus on what you need to grow! üìöüöÄ\n",
            "\n",
            "The future of coding is here, and it‚Äôs powered by AI! Are you ready to embrace these tools and elevate your development skills? Let‚Äôs discuss how AI is shaping your coding journey! üí¨üëá #AI #Coding #JuniorDevelopers #TechInnovation #SkillDevelopment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 8: Memory Management (The Context)\n",
        "\n",
        "**The Problem:** LLMs are stateless. If you say \"Hi, I'm Raja\" and then ask \"What is my name?\", the model will say \"I don't know.\"\n",
        "\n",
        "**The Solution:** **Conversation History**. We must store the back-and-forth messages and inject them into the prompt every time we talk to the model.\n",
        "\n",
        "**The Scenario:** A simple chat bot that remembers your name and preferences."
      ],
      "metadata": {
        "id": "t-Yq8YKRFvWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "# --- Define the Prompt with History ---\n",
        "# MessagesPlaceholder is where the memory will be injected\n",
        "memory_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "chain = memory_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- Setup the Memory Store ---\n",
        "# In production, this would be a database (Redis, Postgres etc).\n",
        "# Here, we use an in-memory dictionary for simplicity.\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "# --- Wrap the Chain with History ---\n",
        "conversation = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# --- Execution ---\n",
        "session_id = \"user_123\"\n",
        "\n",
        "# Turn 1\n",
        "print(\"üó£Ô∏è Turn 1:\")\n",
        "response1 = conversation.invoke(\n",
        "    {\"input\": \"Hi, my name is Raja. I like Python.\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(response1)\n",
        "\n",
        "# Turn 2 (The test)\n",
        "print(\"\\nüó£Ô∏è Turn 2:\")\n",
        "response2 = conversation.invoke(\n",
        "    {\"input\": \"What is my name and what language do I like?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV5hE0mnFyPr",
        "outputId": "a2a7be8d-7f62-42e4-f672-6d63eb0129f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üó£Ô∏è Turn 1:\n",
            "Hi Raja! It's great to hear that you like Python. It's a versatile and powerful programming language. What do you enjoy most about Python? Are you working on any specific projects or learning something new?\n",
            "\n",
            "üó£Ô∏è Turn 2:\n",
            "Your name is Raja, and you like Python.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 9: Learning (Few-Shot Adaptation)\n",
        "\n",
        "**The Problem:** How do you get an agent to follow a very specific format or tone without writing a 10-page instruction manual?\n",
        "\n",
        "**The Solution:** **Few-Shot Learning (In-Context Learning)**. Instead of *telling* the model what to do, you *show* it examples of what you want. The model \"learns\" the pattern from the context.\n",
        "\n",
        "**The Scenario:** We want an agent that converts normal English into \"Pirate Speak\" but specifically in a JSON format."
      ],
      "metadata": {
        "id": "yk16kSoxGKWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "# --- 1. Define Examples (The \"Training\" Data) ---\n",
        "examples = [\n",
        "    {\"input\": \"Hello, how are you?\", \"output\": \"{\\\"text\\\": \\\"Ahoy matey! How be ye?\\\"}\"},\n",
        "    {\"input\": \"Where is the bathroom?\", \"output\": \"{\\\"text\\\": \\\"Avast! Where be the head?\\\"}\"},\n",
        "    {\"input\": \"I am hungry.\", \"output\": \"{\\\"text\\\": \\\"Me belly be rumblin' for grub!\\\"}\"}\n",
        "]\n",
        "\n",
        "# --- 2. Create the Example Prompt ---\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- 3. Create the Few-Shot Prompt ---\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "# --- 4. Assemble the Final Prompt ---\n",
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a pirate. Output ONLY JSON.\"),\n",
        "        few_shot_prompt, # Inject the examples here\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- Execution ---\n",
        "learning_chain = final_prompt | llm | StrOutputParser()\n",
        "\n",
        "print(\"--- Testing Adaptation ---\")\n",
        "# We give a new input, and it should follow the JSON + Pirate pattern\n",
        "result = learning_chain.invoke({\"input\": \"The weather is nice today.\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBRBGOYTGMay",
        "outputId": "d828f27b-da63-4659-9926-cf8c9924f922"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing Adaptation ---\n",
            "{\"text\": \"Aye, the sun be shinin' bright on the high seas!\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "You have moved beyond basic scripts. Your agents now have:\n",
        "\n",
        "1.  **Teamwork:** They can specialize and collaborate.\n",
        "2.  **Memory:** They maintain context over time.\n",
        "3.  **Adaptability:** They learn patterns from examples.\n",
        "\n",
        "In **Part 4**, we will look at **Infrastructure**. We will explore the **Model Context Protocol (MCP)** and how to handle errors when things go wrong."
      ],
      "metadata": {
        "id": "21_qIZX1GeQB"
      }
    }
  ]
}