{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1X5p7oDJx3dOHUctIK3pM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-jamwal/blog-agentic-architectures/blob/main/Part_2_Reflection%2C_Tool_Use%2C_and_Planning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Agents in Python and n8n in 2026\n",
        "Companion to https://rajajamwal.substack.com/p/building-agents-in-python-and-n8n\n",
        "\n",
        "Subscribe to my blog, https://rajajamwal.substack.com"
      ],
      "metadata": {
        "id": "ON9sj85uABbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection, Tool Use, and Planning\n",
        "\n",
        "Welcome to Part 2 of the **Agentic Design Patterns** series.\n",
        "\n",
        "In Part 1, we learned how to structure the flow of data (Chaining, Routing, Parallelization). Now, we are going to give our agents **brains** and **hands**. We are moving from simple pipelines to systems that can think about their work and interact with the outside world.\n",
        "\n",
        "We will cover:\n",
        "1.  **Reflection:** The ability to critique and improve one's own work.\n",
        "2.  **Tool Use:** Connecting the LLM to external functions (APIs, Calculators).\n",
        "3.  **Planning:** Breaking down complex goals into executable steps.\n",
        "\n",
        "### The Stack\n",
        "*   **Python**\n",
        "*   **LangChain**\n",
        "*   **OpenAI** (GPT-4o-mini)\n",
        "\n",
        "### The n8n Connection\n",
        "*   **Reflection** = Looping the output of an AI node back into its input (often with an `If` node to prevent infinite loops).\n",
        "*   **Tool Use** = The **AI Agent Node** in n8n, which allows you to drag-and-drop tools like \"Calculator\" or \"HTTP Request\".\n",
        "*   **Planning** = Using an LLM to generate a JSON list of tasks, then using the `Split In Batches` node to execute them one by one."
      ],
      "metadata": {
        "id": "LmZ5RWkyAG7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Dependencies\n",
        "!pip install -qU langchain langchain-openai langchain-core\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# @title 2. Setup API Key\n",
        "# We need to set this again as Colab runtimes reset\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Initialize the Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# We use a low temperature for reasoning tasks\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "print(\"✅ Environment Setup Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QarVlfDAQEX",
        "outputId": "2f28528e-b383-42a4-db44-43ca7e0475a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/489.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/489.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your OpenAI API Key: ··········\n",
            "✅ Environment Setup Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 4: Reflection (The Self-Correction Loop)\n",
        "\n",
        "**The Problem:** LLMs are \"feed-forward.\" They generate text token-by-token and don't look back. This often leads to code with bugs or writing that misses the point.\n",
        "\n",
        "**The Solution:** Implement a **Generator-Critic** workflow. One step generates the content, and a second step critiques it. Ideally, a third step revises the content based on the critique.\n",
        "\n",
        "**The Scenario:** We want to write a Python function. Instead of trusting the first draft, we will ask the model to critique its own code and then fix it."
      ],
      "metadata": {
        "id": "B8tZeSGcCKHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Reflection Implementation\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- Step 1: The Generator (Draft) ---\n",
        "generator_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a Python function to check if a number is prime. Return ONLY the code.\"\n",
        ")\n",
        "generator_chain = generator_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- Step 2: The Critic (Reflection) ---\n",
        "critic_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a Senior Software Engineer. Critique the following code.\n",
        "    Look for:\n",
        "    1. Efficiency issues.\n",
        "    2. Edge cases (negative numbers, 0, 1).\n",
        "    3. Syntax errors.\n",
        "\n",
        "    Code:\n",
        "    {code}\n",
        "\n",
        "    Provide a concise critique.\"\"\"\n",
        ")\n",
        "critic_chain = critic_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- Step 3: The Revisor (Improvement) ---\n",
        "revise_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Rewrite the following code based on the critique.\n",
        "    Original Code: {code}\n",
        "    Critique: {critique}\n",
        "\n",
        "    Return ONLY the improved code.\"\"\"\n",
        ")\n",
        "revisor_chain = revise_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- Execution Flow ---\n",
        "print(\"1️⃣ Generating Draft...\")\n",
        "draft_code = generator_chain.invoke({})\n",
        "print(f\"Draft:\\n{draft_code}\\n\")\n",
        "\n",
        "print(\"2️⃣ Critiquing...\")\n",
        "critique = critic_chain.invoke({\"code\": draft_code})\n",
        "print(f\"Feedback:\\n{critique}\\n\")\n",
        "\n",
        "print(\"3️⃣ Revising...\")\n",
        "final_code = revisor_chain.invoke({\"code\": draft_code, \"critique\": critique})\n",
        "print(f\"Final Code:\\n{final_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxgaQvbwCK40",
        "outputId": "c0d6669a-6385-4138-9248-20fd436dd369"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1️⃣ Generating Draft...\n",
            "Draft:\n",
            "```python\n",
            "def is_prime(n):\n",
            "    if n <= 1:\n",
            "        return False\n",
            "    for i in range(2, int(n**0.5) + 1):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "```\n",
            "\n",
            "2️⃣ Critiquing...\n",
            "Feedback:\n",
            "The provided code for the `is_prime` function is generally well-structured, but there are a few areas for improvement regarding efficiency, edge cases, and syntax.\n",
            "\n",
            "### 1. Efficiency Issues:\n",
            "- The current implementation checks all numbers from 2 to the square root of `n`. This is efficient for most cases, but it can be further optimized:\n",
            "  - After checking for divisibility by 2, you can skip all even numbers by iterating through odd numbers only (i.e., starting from 3 and incrementing by 2). This reduces the number of iterations by about half for larger numbers.\n",
            "\n",
            "### 2. Edge Cases:\n",
            "- The function correctly handles negative numbers and zero by returning `False` for `n <= 1`.\n",
            "- It also correctly identifies `1` as not prime. However, it does not explicitly handle the case of `2`, which is the smallest and only even prime number. While it will return `True` for `2`, it could be beneficial to add a specific check for `2` to improve clarity and performance.\n",
            "\n",
            "### 3. Syntax Errors:\n",
            "- There are no syntax errors in the code. It is syntactically correct and will run without issues.\n",
            "\n",
            "### Suggested Improvements:\n",
            "Here’s a revised version of the function incorporating the suggestions:\n",
            "\n",
            "```python\n",
            "def is_prime(n):\n",
            "    if n <= 1:\n",
            "        return False\n",
            "    if n == 2:\n",
            "        return True  # Handle the case for 2 explicitly\n",
            "    if n % 2 == 0:\n",
            "        return False  # Exclude all even numbers greater than 2\n",
            "    for i in range(3, int(n**0.5) + 1, 2):  # Check only odd numbers\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "```\n",
            "\n",
            "### Summary:\n",
            "- The original code is functional but can be optimized for efficiency by skipping even numbers after checking for `2`.\n",
            "- It correctly handles edge cases for negative numbers, zero, and one, but could benefit from an explicit check for `2`.\n",
            "- There are no syntax errors present.\n",
            "\n",
            "3️⃣ Revising...\n",
            "Final Code:\n",
            "```python\n",
            "def is_prime(n):\n",
            "    if n <= 1:\n",
            "        return False\n",
            "    if n == 2:\n",
            "        return True  # Handle the case for 2 explicitly\n",
            "    if n % 2 == 0:\n",
            "        return False  # Exclude all even numbers greater than 2\n",
            "    for i in range(3, int(n**0.5) + 1, 2):  # Check only odd numbers\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 5: Tool Use (Function Calling)\n",
        "\n",
        "**The Problem:** LLMs are trapped in a text box. They can't calculate accurate math, check the weather, or query a database.\n",
        "\n",
        "**The Solution:** **Tool Binding**. We describe a function to the LLM (inputs, outputs, description). The LLM doesn't run the code; it decides *which* tool to call and *what* arguments to pass. The system then runs the tool.\n",
        "\n",
        "**The Scenario:** We will give the LLM a simple \"Calculator\" tool (since LLMs are notoriously bad at math) and ask it a math question."
      ],
      "metadata": {
        "id": "ynEkYau7CrgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# --- Define the Tool ---\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies two integers together.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# --- Bind Tool to Model ---\n",
        "# This tells the LLM: \"You have this tool available if you need it.\"\n",
        "llm_with_tools = llm.bind_tools([multiply])\n",
        "\n",
        "# --- Execution ---\n",
        "query = \"What is 45 multiplied by 12?\"\n",
        "response = llm_with_tools.invoke(query)\n",
        "\n",
        "# --- Inspecting the Result ---\n",
        "# The model does NOT return text. It returns a 'tool_call' object.\n",
        "print(f\"User Query: {query}\")\n",
        "print(f\"AI Decision: {response.tool_calls}\")\n",
        "\n",
        "# Note: In a full agent loop (like LangGraph or n8n), we wille explore LanGraph in another series, the system would now:\n",
        "# 1. Take this tool call.\n",
        "# 2. Execute the Python function `multiply(45, 12)`.\n",
        "# 3. Feed the result (540) back to the LLM to generate the final answer."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwU_V5EFCwwi",
        "outputId": "9a6e9fbd-0713-4d7e-d3e8-088dabbc313b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: What is 45 multiplied by 12?\n",
            "AI Decision: [{'name': 'multiply', 'args': {'a': 45, 'b': 12}, 'id': 'call_r2zTriiFlmRfyZ7PPOrokr2w', 'type': 'tool_call'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pattern 6: Planning (The Architect)\n",
        "\n",
        "**The Problem:** If you ask an agent to \"Build a website,\" it will fail. The task is too big. It needs to be broken down.\n",
        "\n",
        "**The Solution:** **Chain of Thought Planning**. Before executing, the agent generates a step-by-step plan. This plan serves as a roadmap for subsequent execution steps.\n",
        "\n",
        "**The Scenario:** We want to write a technical blog post. Instead of writing it all at once, we will first generate a **Plan (Outline)**, and then (conceptually) we would execute each step."
      ],
      "metadata": {
        "id": "wSroifUlDI2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- The Planner ---\n",
        "planner_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a Content Strategist.\n",
        "    Create a numbered step-by-step plan to write a blog post about: {topic}.\n",
        "    The plan should have exactly 4 steps.\n",
        "    Return ONLY the list.\"\"\"\n",
        ")\n",
        "\n",
        "planner_chain = planner_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- The Executor (Simulator) ---\n",
        "# In a real agent, this would be a loop. Here, we simulate the first step.\n",
        "executor_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a Writer. Execute Step 1 of this plan:\n",
        "    Plan: {plan}\n",
        "\n",
        "    Write the content for Step 1 only.\"\"\"\n",
        ")\n",
        "executor_chain = executor_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- Execution ---\n",
        "topic = \"The Future of AI Agents\"\n",
        "\n",
        "print(\"1️⃣ Creating Plan...\")\n",
        "plan = planner_chain.invoke({\"topic\": topic})\n",
        "print(f\"Plan Generated:\\n{plan}\\n\")\n",
        "\n",
        "print(\"2️⃣ Executing Step 1...\")\n",
        "step_1_content = executor_chain.invoke({\"plan\": plan})\n",
        "print(f\"Result of Step 1:\\n{step_1_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4YD3fSaDJ06",
        "outputId": "8b9a5bea-6feb-483c-96bc-63078819c98e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1️⃣ Creating Plan...\n",
            "Plan Generated:\n",
            "1. **Research and Gather Information**: Explore current trends, advancements, and predictions related to AI agents by reviewing academic articles, industry reports, and expert opinions.\n",
            "\n",
            "2. **Outline the Blog Post**: Create a structured outline that includes an introduction, key sections (such as definitions, current applications, future predictions, and ethical considerations), and a conclusion.\n",
            "\n",
            "3. **Draft the Content**: Write the blog post based on the outline, ensuring to incorporate engaging language, relevant examples, and clear explanations to make complex concepts accessible to readers.\n",
            "\n",
            "4. **Edit and Optimize for SEO**: Review the draft for clarity, coherence, and grammatical accuracy, then optimize it for search engines by including relevant keywords, meta descriptions, and internal/external links.\n",
            "\n",
            "2️⃣ Executing Step 1...\n",
            "Result of Step 1:\n",
            "### Step 1: Research and Gather Information on AI Agents\n",
            "\n",
            "To effectively explore the current landscape of AI agents, I will delve into various sources, including academic articles, industry reports, and expert opinions. Here’s a summary of the key findings from this research:\n",
            "\n",
            "#### Current Trends in AI Agents\n",
            "\n",
            "1. **Natural Language Processing (NLP)**:\n",
            "   - Recent advancements in NLP have significantly improved the ability of AI agents to understand and generate human language. Technologies like OpenAI's GPT-3 and Google's BERT have set new benchmarks in conversational AI, enabling more fluid and context-aware interactions.\n",
            "\n",
            "2. **Autonomous Agents**:\n",
            "   - AI agents are increasingly being deployed in autonomous systems, such as self-driving cars and drones. Companies like Tesla and Waymo are at the forefront, utilizing AI to enhance safety and efficiency in transportation.\n",
            "\n",
            "3. **Personal Assistants**:\n",
            "   - Virtual assistants like Amazon's Alexa, Apple's Siri, and Google Assistant are becoming more integrated into daily life. They are evolving from simple task performers to more sophisticated agents capable of managing complex workflows and providing personalized recommendations.\n",
            "\n",
            "4. **AI in Customer Service**:\n",
            "   - Businesses are leveraging AI agents for customer support, utilizing chatbots and virtual agents to handle inquiries, troubleshoot issues, and provide 24/7 assistance. This trend is driven by the need for cost-effective solutions and improved customer experience.\n",
            "\n",
            "#### Advancements in AI Technology\n",
            "\n",
            "1. **Machine Learning and Deep Learning**:\n",
            "   - The use of machine learning algorithms, particularly deep learning, has enabled AI agents to learn from vast amounts of data, improving their performance over time. This is particularly evident in image and speech recognition applications.\n",
            "\n",
            "2. **Reinforcement Learning**:\n",
            "   - Reinforcement learning is gaining traction in training AI agents to make decisions in dynamic environments. This approach is being applied in gaming, robotics, and resource management, allowing agents to optimize their actions based on feedback from their environment.\n",
            "\n",
            "3. **Explainable AI (XAI)**:\n",
            "   - As AI agents become more prevalent, the demand for transparency in their decision-making processes has led to the development of explainable AI. This field focuses on creating models that can provide insights into how decisions are made, which is crucial for trust and accountability.\n",
            "\n",
            "#### Predictions for the Future\n",
            "\n",
            "1. **Increased Autonomy**:\n",
            "   - Experts predict that AI agents will become more autonomous, capable of performing complex tasks without human intervention. This could lead to significant advancements in industries such as healthcare, logistics, and manufacturing.\n",
            "\n",
            "2. **Human-AI Collaboration**:\n",
            "   - The future will likely see a shift towards collaborative AI, where human workers and AI agents work together to enhance productivity. This partnership could redefine job roles and create new opportunities in various sectors.\n",
            "\n",
            "3. **Ethical and Regulatory Frameworks**:\n",
            "   - As AI agents become more integrated into society, there will be a growing need for ethical guidelines and regulatory frameworks to address concerns related to privacy, bias, and accountability. Policymakers and industry leaders will need to collaborate to establish standards that ensure responsible AI development.\n",
            "\n",
            "#### Ethical Considerations\n",
            "\n",
            "1. **Bias and Fairness**:\n",
            "   - One of the significant challenges facing AI agents is the potential for bias in algorithms, which can lead to unfair treatment of individuals or groups. Ongoing research is focused on identifying and mitigating these biases to ensure equitable outcomes.\n",
            "\n",
            "2. **Privacy Concerns**:\n",
            "   - The deployment of AI agents often involves the collection and analysis of personal data, raising concerns about privacy and data security. Striking a balance between innovation and user privacy will be crucial in the development of future AI technologies.\n",
            "\n",
            "3. **Job Displacement**:\n",
            "   - The rise of AI agents has sparked debates about job displacement and the future of work. While AI can enhance efficiency, there are concerns about the impact on employment and the need for reskilling the workforce.\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "The research highlights the dynamic and rapidly evolving field of AI agents, showcasing their current applications, technological advancements, and future potential. As we move forward, it will be essential to address the ethical implications and ensure that AI agents are developed and deployed responsibly. This foundational understanding will serve as the basis for outlining the blog post in the next step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "You have now added cognitive superpowers to your agent:\n",
        "\n",
        "1.  **Reflection:** It can fix its own mistakes.\n",
        "2.  **Tool Use:** It knows how to ask for help (calculators, APIs).\n",
        "3.  **Planning:** It can break big mountains into climbable hills.\n",
        "\n",
        "In **Part 3**, we will scale this up. We will look at **Multi-Agent Collaboration** (teams of agents) and **Memory** (remembering the conversation)."
      ],
      "metadata": {
        "id": "70Mq_JoID4h7"
      }
    }
  ]
}