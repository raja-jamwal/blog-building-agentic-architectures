{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-jamwal/blog-building-agentic-architectures/blob/main/langchain/Part_5_Human_in_the_Loop%2C_RAG%2C_and_Inter_Agent_Communication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLxsLkYXdF9v"
      },
      "source": [
        "# Building Agents in Python and n8n in 2026\n",
        "Companion to https://rajajamwal.substack.com/p/building-agents-in-python-and-n8n\n",
        "\n",
        "Subscribe to my blog, https://rajajamwal.substack.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8h_RwTkdLez"
      },
      "source": [
        "## Human-in-the-Loop, RAG, and Inter-Agent Communication\n",
        "\n",
        "We have built robust, goal-oriented agents. But in the real world, we cannot always trust an AI to act alone (especially with sensitive data), nor does the AI know everything.\n",
        "\n",
        "Today, we focus on **Trust** and **Connection**.\n",
        "\n",
        "We will cover:\n",
        "1.  **Human-in-the-Loop (HITL):** Pausing the AI to get human approval before taking action.\n",
        "2.  **Knowledge Retrieval (RAG):** Giving the AI access to private data (PDFs, Docs) so it doesn't hallucinate.\n",
        "3.  **Inter-Agent Communication (A2A):** How one agent can \"hire\" another agent to do a task.\n",
        "\n",
        "### The Stack\n",
        "*   **Python**\n",
        "*   **LangChain**\n",
        "*   **OpenAI** (GPT-4o-mini)\n",
        "*   **Vector Store** (In-Memory for demo)\n",
        "\n",
        "\n",
        "### The n8n Connection\n",
        "*   **HITL** = The **Wait Node**. You can configure it to \"Wait for Webhook\" (e.g., a button click in an email) before the workflow continues.\n",
        "*   **RAG** = The **Vector Store** nodes (Pinecone, Qdrant, Supabase). You can drag a \"Retrieve from Vector Store\" node to inject context into your AI Agent.\n",
        "*   **A2A** = **Webhooks**. Agent A uses an HTTP Request node to send data to Agent B's Webhook URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzI1UxOQdbEA",
        "outputId": "834ea853-1c0e-45a1-febb-011594eec018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/84.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mEnter your OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ… Environment Setup Complete.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies\n",
        "# We need langchain-community and a vector store library for RAG\n",
        "!pip install -qU langchain langchain-openai langchain-community faiss-cpu\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# @title 2. Setup API Key\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Initialize the Model\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "print(\"âœ… Environment Setup Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sao8VDq1dr0I"
      },
      "source": [
        "## Pattern 13: Human-in-the-Loop (HITL)\n",
        "\n",
        "**The Problem:** You want an agent to draft emails to clients, but you are terrified it might say something offensive or incorrect. You can't let it run fully autonomous.\n",
        "\n",
        "**The Solution:** **The Approval Gate**. The agent does the heavy lifting (drafting), but pauses execution to ask a human for a \"Go/No-Go\" decision.\n",
        "\n",
        "**The Scenario:** An agent drafts a refund email. We (the human) must approve it before it is \"sent.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8qWSGVEdu9c",
        "outputId": "858bce82-2c85-410d-9276-8a26860b615e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¤– Agent is working...\n",
            "\n",
            "--- ðŸ“ DRAFT GENERATED FOR ALICE ---\n",
            "Subject: Refund Request Update\n",
            "\n",
            "Dear Alice,\n",
            "\n",
            "Thank you for reaching out. Unfortunately, we cannot process your refund request as it has been over 30 days since your purchase. We appreciate your understanding and are here to assist you with any other questions.\n",
            "\n",
            "Best regards,  \n",
            "[Your Name]  \n",
            "[Your Company]  \n",
            "------------------------------------------\n",
            "Do you approve this email? (yes/no): yes\n",
            "\n",
            "âœ… Email SENT successfully.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- Step 1: The Drafter ---\n",
        "draft_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Draft a polite email to a customer named {name} denying their refund request because it has been over 30 days. Keep it under 50 words.\"\n",
        ")\n",
        "chain = draft_prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- Execution with Human Intervention ---\n",
        "customer = \"Alice\"\n",
        "\n",
        "print(\"ðŸ¤– Agent is working...\")\n",
        "draft_email = chain.invoke({\"name\": customer})\n",
        "\n",
        "print(f\"\\n--- ðŸ“ DRAFT GENERATED FOR {customer.upper()} ---\")\n",
        "print(draft_email)\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "# --- The \"Loop\" (Python Input) ---\n",
        "# In n8n, this would be a 'Wait' node sending you a Slack message with buttons.\n",
        "user_approval = input(\"Do you approve this email? (yes/no): \")\n",
        "\n",
        "if user_approval.lower() == \"yes\":\n",
        "    print(\"\\nâœ… Email SENT successfully.\")\n",
        "    # Code to send email would go here\n",
        "else:\n",
        "    print(\"\\nðŸ›‘ Email ABORTED. Please refine the prompt.\")\n",
        "    feedback = input(\"Enter feedback for the agent: \")\n",
        "    # In a real system, you would loop back to the Drafter with this feedback.\n",
        "    print(f\"Feedback '{feedback}' logged for next iteration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7A8JpQxd8ig"
      },
      "source": [
        "## Pattern 14: Knowledge Retrieval (RAG)\n",
        "\n",
        "**The Problem:** LLMs only know what they were trained on. They don't know your company's internal HR policy or your personal notes.\n",
        "\n",
        "**The Solution:** **RAG (Retrieval-Augmented Generation)**.\n",
        "1.  **Ingest:** Break your documents into chunks and save them in a Vector Database.\n",
        "2.  **Retrieve:** When a user asks a question, find the most relevant chunks.\n",
        "3.  **Generate:** Send the chunks + the question to the LLM.\n",
        "\n",
        "**The Scenario:** We will create a mini \"Company Policy\" database and ask the agent questions about it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5-rsR7AeAtG",
        "outputId": "eaf20474-66d2-4e3a-9533-4dc91dff14bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Query 1: Remote Work ---\n",
            "You can work from home for 20 days per year.\n",
            "\n",
            "--- Query 2: Kitchen Policy ---\n",
            "No, the office kitchen is closed for cleaning on Fridays.\n",
            "\n",
            "--- Query 3: Unknown Info (Hallucination Check) ---\n",
            "The provided context does not contain any information about the stock price.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# --- 1. The Knowledge Base (Simulated) ---\n",
        "documents = [\n",
        "    \"Employees are allowed 20 days of remote work per year.\",\n",
        "    \"Expense reports must be submitted by the 5th of each month.\",\n",
        "    \"The office kitchen is closed for cleaning on Fridays.\",\n",
        "    \"The CEO's name is Raja.\"\n",
        "]\n",
        "\n",
        "# --- 2. Ingest (Create Vector Store) ---\n",
        "# We turn text into numbers (embeddings) so we can search by meaning.\n",
        "vectorstore = FAISS.from_texts(documents, embedding=embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# --- 3. The RAG Prompt ---\n",
        "template = \"\"\"Answer the question based ONLY on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "rag_prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# --- 4. The RAG Chain ---\n",
        "# RunnablePassthrough passes the question to the retriever AND the prompt\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- Execution ---\n",
        "print(\"--- Query 1: Remote Work ---\")\n",
        "print(rag_chain.invoke(\"How many days can I work from home?\"))\n",
        "\n",
        "print(\"\\n--- Query 2: Kitchen Policy ---\")\n",
        "print(rag_chain.invoke(\"Can I use the kitchen on Friday?\"))\n",
        "\n",
        "print(\"\\n--- Query 3: Unknown Info (Hallucination Check) ---\")\n",
        "# It should say it doesn't know, or answer based only on context.\n",
        "print(rag_chain.invoke(\"What is the stock price?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER7vjlGbefeQ"
      },
      "source": [
        "## Pattern 15: Inter-Agent Communication (A2A)\n",
        "\n",
        "**The Problem:** You have a \"Weather Agent\" built in Python and a \"Travel Agent\" built in n8n. How do they talk?\n",
        "\n",
        "**The Solution:** **Standardized Protocols (Agent Cards)**. Agents should expose their capabilities via APIs (Tools). One agent can \"call\" another agent just like it calls a calculator.\n",
        "\n",
        "**The Scenario:** Our Main Agent needs to get data from a \"Remote Weather Bot\" (simulated here as a tool)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oA5Crvxegls",
        "outputId": "dabe9eb7-689e-4315-d75d-27a28f62fc7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Main Agent Decision ---\n",
            "[{'name': 'call_remote_weather_agent', 'args': {'city': 'Tokyo'}, 'id': 'call_ixHIc2PTs6yCxDd05QFEY8Qx', 'type': 'tool_call'}]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# --- The \"Remote\" Agent ---\n",
        "# Imagine this function is actually an API call to http://weather-agent.com/api\n",
        "@tool\n",
        "def call_remote_weather_agent(city: str) -> str:\n",
        "    \"\"\"Call this to get weather info from the specialized WeatherBot.\"\"\"\n",
        "    print(f\"ðŸ“¡ Connecting to Remote WeatherBot for {city}...\")\n",
        "    # Simulating a JSON response from a remote agent\n",
        "    return f\"{{'agent': 'WeatherBot v2', 'city': '{city}', 'temp': '24C', 'condition': 'Sunny'}}\"\n",
        "\n",
        "# --- The Main Agent ---\n",
        "tools = [call_remote_weather_agent]\n",
        "main_agent = llm.bind_tools(tools)\n",
        "\n",
        "# --- Execution ---\n",
        "query = \"I am planning a trip to Tokyo. Ask the weather bot what it's like there.\"\n",
        "response = main_agent.invoke(query)\n",
        "\n",
        "print(\"\\n--- Main Agent Decision ---\")\n",
        "print(response.tool_calls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey7xnypZe2nN"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Your agents are now connected to reality:\n",
        "\n",
        "1.  **HITL:** They respect human authority.\n",
        "2.  **RAG:** They know your private data.\n",
        "3.  **A2A:** They can collaborate with other systems.\n",
        "\n",
        "In **Part 6**, we will optimize. We will look at **Resource-Awareness** (saving money by switching models) and **Guardrails** (ensuring safety)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOGECnel3GEiNxaGwhyGETi",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
