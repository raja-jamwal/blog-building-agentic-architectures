{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1hDq55K3LKjKDcWcKOYSg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-jamwal/blog-agentic-architectures/blob/main/langgraph
/Part_1_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Agents in Python and n8n in 2026\n",
        "Companion to https://rajajamwal.substack.com/p/building-agents-in-python-and-n8n\n",
        "\n",
        "Subscribe to my blog, https://rajajamwal.substack.com"
      ],
      "metadata": {
        "id": "SVo8JCFi4dLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Chains to Loops: Building a Self-Correcting Agent\n",
        "\n",
        "In previous parts, we used **LangChain** to build linear workflows (Chains).\n",
        "Now, we are moving to **LangGraph** to build **State Machines**.\n",
        "\n",
        "Why? Because real agents need **Loops**.\n",
        "*   They need to try, fail, and retry (Self-Correction).\n",
        "*   They need to pause and wait for a human (Human-in-the-Loop).\n",
        "*   They need to maintain state over long periods (Memory).\n",
        "\n",
        "In this notebook, we will build a single **\"Super-Node\" Agent** that consolidates 7 design patterns into one architecture.\n",
        "\n",
        "### The Stack\n",
        "*   **Python**\n",
        "*   **LangGraph:** For stateful orchestration.\n",
        "*   **LangChain:** For tool definitions and model wrapping.\n",
        "*   **OpenAI:** GPT-4o-mini.\n",
        "\n",
        "### The Goal\n",
        "Build an agent that:\n",
        "1.  **Plans** a solution.\n",
        "2.  **Uses Tools** to execute it.\n",
        "3.  **Pauses** before performing dangerous actions (HITL).\n",
        "4.  **Loops** back to the start to verify its work."
      ],
      "metadata": {
        "id": "ji62lAi-4jqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Dependencies\n",
        "# We need langgraph specifically for this architecture\n",
        "!pip install -qU langgraph langchain langchain-openai langchain-core\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# @title 2. Setup API Key\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Initialize the Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# We use temperature=0 for strict adherence to instructions\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "print(\"‚úÖ Environment Setup Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaBGJpQJ4sgm",
        "outputId": "39a6aa58-6a56-4c7a-ded2-98b134ecce9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/84.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/489.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m481.3/489.1 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Environment Setup Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Defining the State (The Memory)\n",
        "\n",
        "In LangChain, memory is often a hidden wrapper. In LangGraph, memory is **explicit**.\n",
        "\n",
        "We define a `State` object. This is the \"Shared Brain\" of the agent.\n",
        "*   `messages`: A list of all messages in the conversation.\n",
        "*   `add_messages`: A special reducer that tells LangGraph to *append* new messages to the list rather than overwriting them."
      ],
      "metadata": {
        "id": "ACHg4y4c5Chz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# This is the schema of our agent's brain\n",
        "class AgentState(TypedDict):\n",
        "    # 'add_messages' ensures that when a node returns a message,\n",
        "    # it is added to this list, preserving history.\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "print(\"‚úÖ State Defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfNoPqll5DMI",
        "outputId": "5cc9f03e-1e2d-48f4-ee77-d18269ef3cbd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ State Defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Defining Tools (The Hands)\n",
        "\n",
        "We will give the agent two tools:\n",
        "1.  `search_user`: A safe, read-only tool.\n",
        "2.  `delete_user`: A **dangerous** tool. We will use this to demonstrate the **Human-in-the-Loop (HITL)** pattern later."
      ],
      "metadata": {
        "id": "QS2pGEwM5fyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def search_user(query: str):\n",
        "    \"\"\"Searches for a user's ID based on their name.\"\"\"\n",
        "    # Mock database lookup\n",
        "    if \"raja\" in query.lower():\n",
        "        return \"User ID: 998877\"\n",
        "    return \"User not found.\"\n",
        "\n",
        "@tool\n",
        "def delete_user(user_id: str):\n",
        "    \"\"\"DELETES a user from the database. DANGEROUS ACTION.\"\"\"\n",
        "    # Mock deletion\n",
        "    return f\"SUCCESS: User {user_id} has been permanently deleted.\"\n",
        "\n",
        "# Bind tools to the LLM\n",
        "# This gives the LLM the *ability* to call them, but doesn't execute them yet.\n",
        "tools = [search_user, delete_user]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "print(\"‚úÖ Tools Created and Bound.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9h0gcd15gLb",
        "outputId": "3578682e-aaa5-486b-fdae-e7d5c2b29e70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tools Created and Bound.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: The Nodes (The Brain & The Executor)\n",
        "\n",
        "A Graph consists of Nodes. We need two:\n",
        "\n",
        "1.  **Agent Node:** The Reasoner. It looks at the state (history) and decides what to do next.\n",
        "2.  **Tool Node:** The Executor. It actually runs the Python functions if the Agent requested them."
      ],
      "metadata": {
        "id": "bOmqgRlA5x3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# Node 1: The Agent (Reasoner)\n",
        "def agent_node(state: AgentState):\n",
        "    # We take the current history\n",
        "    messages = state[\"messages\"]\n",
        "    # We ask the LLM what to do\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    # We return the response, which LangGraph adds to the state\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Node 2: The Tool Executor\n",
        "# LangGraph provides a prebuilt node for this!\n",
        "# It automatically parses the LLM's tool calls and runs the functions.\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "print(\"‚úÖ Nodes Defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoL27E8I5yf0",
        "outputId": "8fdb0302-acc2-4cc8-de6f-5097b7617b3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Nodes Defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: The Graph Architecture (The Loop)\n",
        "\n",
        "This is where the magic happens. We wire the nodes together.\n",
        "\n",
        "**The Logic:**\n",
        "1.  Start at `agent`.\n",
        "2.  Check the output (`should_continue`):\n",
        "    *   Did the agent call a tool? -> Go to `tools`.\n",
        "    *   Did the agent just speak? -> Go to `END`.\n",
        "3.  **The Loop:** If we went to `tools`, we go **BACK** to `agent` afterwards. This allows the agent to see the tool output and decide what to do next (Reflection/Self-Correction)."
      ],
      "metadata": {
        "id": "qsLnoNbo53go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# 1. Initialize Graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# 2. Add Nodes\n",
        "workflow.add_node(\"agent\", agent_node)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "# 3. Define Edges\n",
        "workflow.add_edge(START, \"agent\")\n",
        "\n",
        "# 4. Define Conditional Logic (Routing)\n",
        "def should_continue(state: AgentState):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    # If the LLM generated a tool call, go to the tool node\n",
        "    if last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    # Otherwise, stop\n",
        "    return END\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    [\"tools\", END]\n",
        ")\n",
        "\n",
        "# 5. The Critical Loop\n",
        "# After tools run, feed the result back to the agent\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "print(\"‚úÖ Graph Wired.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mMBSOHE533K",
        "outputId": "620d2117-373d-42ef-8034-d8d68a3066ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Graph Wired.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Human-in-the-Loop (HITL)\n",
        "\n",
        "We don't want the agent deleting users without permission.\n",
        "\n",
        "In LangGraph, we add a **Checkpointer** (Memory) and set an **interrupt**.\n",
        "`interrupt_before=[\"tools\"]` tells the graph: *\"If you are about to enter the 'tools' node, FREEZE and wait for approval.\"*"
      ],
      "metadata": {
        "id": "VjNJ8aSj6wiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# In-memory storage for the state (in production, use Postgres/Redis)\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Compile the graph with the interrupt\n",
        "app = workflow.compile(\n",
        "    checkpointer=checkpointer,\n",
        "    interrupt_before=[\"tools\"] # <--- THE SAFETY MECHANISM\n",
        ")\n",
        "\n",
        "print(\"‚úÖ App Compiled with Safety Checks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii_wVTDp6w8n",
        "outputId": "e5a3eae9-757c-43f3-e7d8-c520dc39597b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ App Compiled with Safety Checks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Execution\n",
        "\n",
        "Let's run the agent.\n",
        "**Scenario:** \"Find Raja's ID and then delete him.\"\n",
        "\n",
        "Watch what happens:\n",
        "1.  The Agent will search for the ID.\n",
        "2.  It will **PAUSE** before searching (because searching is a tool).\n",
        "3.  We will approve it.\n",
        "4.  It will find the ID.\n",
        "5.  It will try to delete the ID.\n",
        "6.  It will **PAUSE** again.\n",
        "7.  We will approve it."
      ],
      "metadata": {
        "id": "C5f7G19V69fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Config acts as the Session ID\n",
        "config = {\"configurable\": {\"thread_id\": \"session_1\"}}\n",
        "\n",
        "# --- 1. Initial Request ---\n",
        "print(\"--- üó£Ô∏è User Request: Find Raja and Delete him ---\")\n",
        "initial_input = {\"messages\": [(\"user\", \"Find Raja's ID and then delete his record.\")]}\n",
        "\n",
        "# Run the graph until it hits the interrupt\n",
        "for event in app.stream(initial_input, config=config):\n",
        "    for key, value in event.items():\n",
        "        print(f\"-> Node '{key}' executed.\")\n",
        "\n",
        "print(\"\\nüõë GRAPH PAUSED (HITL Triggered) üõë\")\n",
        "\n",
        "# --- 2. Inspect State ---\n",
        "# Let's see what the agent wants to do\n",
        "snapshot = app.get_state(config)\n",
        "next_step = snapshot.next\n",
        "last_message = snapshot.values[\"messages\"][-1]\n",
        "\n",
        "if next_step[0] == \"tools\":\n",
        "    tool_calls = last_message.tool_calls\n",
        "    print(f\"‚ö†Ô∏è Agent wants to execute: {tool_calls[0]['name']}\")\n",
        "    print(f\"Arguments: {tool_calls[0]['args']}\")\n",
        "\n",
        "# --- 3. Human Approval ---\n",
        "approval = input(\"\\nDo you approve this action? (yes/no): \")\n",
        "\n",
        "if approval.lower() == \"yes\":\n",
        "    print(\"\\n‚úÖ Action Approved. Resuming...\")\n",
        "    # Passing None tells LangGraph to resume from the saved state\n",
        "    for event in app.stream(None, config=config):\n",
        "        for key, value in event.items():\n",
        "            print(f\"-> Node '{key}' executed.\")\n",
        "else:\n",
        "    print(\"‚ùå Action Denied.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH1fkBvQ7M66",
        "outputId": "573853ee-c7e7-4d8f-ca20-41fbba7d821e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üó£Ô∏è User Request: Find Raja and Delete him ---\n",
            "-> Node 'agent' executed.\n",
            "-> Node '__interrupt__' executed.\n",
            "\n",
            "üõë GRAPH PAUSED (HITL Triggered) üõë\n",
            "‚ö†Ô∏è Agent wants to execute: search_user\n",
            "Arguments: {'query': 'Raja'}\n",
            "\n",
            "Do you approve this action? (yes/no): yes\n",
            "\n",
            "‚úÖ Action Approved. Resuming...\n",
            "-> Node 'tools' executed.\n",
            "-> Node 'agent' executed.\n",
            "-> Node '__interrupt__' executed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Second Loop\n",
        "\n",
        "If you approved the first action (`search_user`), the agent is now back in the loop. It has the ID. Now it wants to `delete_user`.\n",
        "\n",
        "Because we set `interrupt_before=[\"tools\"]`, it will pause **AGAIN** before the deletion.\n",
        "\n",
        "Run the cell below to handle the second step."
      ],
      "metadata": {
        "id": "MKBgdutR7XPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check state again\n",
        "snapshot = app.get_state(config)\n",
        "\n",
        "if snapshot.next:\n",
        "    print(\"\\nüõë GRAPH PAUSED AGAIN üõë\")\n",
        "    last_message = snapshot.values[\"messages\"][-1]\n",
        "    if last_message.tool_calls:\n",
        "        print(f\"‚ö†Ô∏è Agent wants to execute: {last_message.tool_calls[0]['name']}\")\n",
        "        print(f\"Arguments: {last_message.tool_calls[0]['args']}\")\n",
        "\n",
        "    approval = input(\"\\nDo you approve this DELETION? (yes/no): \")\n",
        "\n",
        "    if approval.lower() == \"yes\":\n",
        "        print(\"\\n‚úÖ Deletion Approved. Resuming...\")\n",
        "        for event in app.stream(None, config=config):\n",
        "            for key, value in event.items():\n",
        "                print(f\"-> Node '{key}' executed.\")\n",
        "\n",
        "        # Final Result\n",
        "        final_snapshot = app.get_state(config)\n",
        "        print(f\"\\nüèÅ Final Agent Response: {final_snapshot.values['messages'][-1].content}\")\n",
        "    else:\n",
        "        print(\"‚ùå Deletion Denied.\")\n",
        "else:\n",
        "    print(\"Graph finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0u02q_-7Y8Z",
        "outputId": "26ac0ad4-c23f-4bd6-9ecb-7f4f84ced00a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üõë GRAPH PAUSED AGAIN üõë\n",
            "‚ö†Ô∏è Agent wants to execute: delete_user\n",
            "Arguments: {'user_id': '998877'}\n",
            "\n",
            "Do you approve this DELETION? (yes/no): yes\n",
            "\n",
            "‚úÖ Deletion Approved. Resuming...\n",
            "-> Node 'tools' executed.\n",
            "-> Node 'agent' executed.\n",
            "\n",
            "üèÅ Final Agent Response: Raja's record has been successfully deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "You have just built a **Stateful, Self-Correcting, Human-in-the-Loop Agent**.\n",
        "\n",
        "Unlike a simple Chain, this Graph:\n",
        "1.  **Maintained Memory** across multiple pauses (`thread_id`).\n",
        "2.  **Looped** back to the agent after every tool call.\n",
        "3.  **Interrupted** execution to ensure safety.\n",
        "\n",
        "This is the architecture used by production agents like Devin or AutoGPT."
      ],
      "metadata": {
        "id": "k7kh1yYi8P6p"
      }
    }
  ]
}
