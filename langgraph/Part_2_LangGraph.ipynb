{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/d7M1KZPOcaLwgUS0w1hT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-jamwal/blog-agentic-architectures/blob/main/Part_2_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Agents in Python and n8n in 2026\n",
        "Companion to https://rajajamwal.substack.com/p/building-agents-in-python-and-n8n\n",
        "\n",
        "Subscribe to my blog, https://rajajamwal.substack.com"
      ],
      "metadata": {
        "id": "baPfjm2KFDha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Orchestrating Intelligence: Building a Multi-Agent Supervisor\n",
        "\n",
        "Welcome to Part 2 of the LangGraph implementation guide.\n",
        "\n",
        "In Part 1, we built a single \"Super-Node\" agent.\n",
        "In Part 2, we are scaling up. We will build a **Multi-Agent System**.\n",
        "\n",
        "### The Architecture: Hub & Spoke\n",
        "Instead of one agent trying to do everything, we will create:\n",
        "1.  **The Supervisor (Hub):** A \"Project Manager\" LLM that plans and delegates.\n",
        "2.  **The Workers (Spokes):** Specialized agents (Researcher, Coder) that execute tasks.\n",
        "\n",
        "### The Stack\n",
        "*   **LangGraph:** For the cyclic graph topology.\n",
        "*   **LangChain:** For agent definitions.\n",
        "*   **OpenAI:** GPT-4o (The Supervisor needs a smart model).\n",
        "\n",
        "### The Goal\n",
        "Handle a complex request: *\"Research the current stock price of Apple and plot a chart.\"*\n",
        "*   The **Researcher** will find the data.\n",
        "*   The **Coder** will generate the plot code.\n",
        "*   The **Supervisor** will manage the handoffs."
      ],
      "metadata": {
        "id": "9QSj6HY6FJDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Dependencies\n",
        "!pip install -qU langgraph langchain langchain-openai langchain-core\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# @title 2. Setup API Key\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Initialize the Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# We use GPT-4o because routing requires high reasoning capabilities\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "print(\"âœ… Environment Setup Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWcM_mTnG5X_",
        "outputId": "37adcf0d-e6d2-4e0a-b2c1-bbf862120898"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/105.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/84.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/489.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ… Environment Setup Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Defining the Team State\n",
        "\n",
        "Our state needs to track two things:\n",
        "1.  `messages`: The global conversation history (so everyone sees what has been done).\n",
        "2.  `next`: A string indicating **who acts next** (e.g., \"Researcher\", \"Coder\", or \"FINISH\")."
      ],
      "metadata": {
        "id": "gERzhHBYCQJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    # The shared memory of the team\n",
        "    messages: Annotated[list, add_messages]\n",
        "    # The supervisor's decision on who acts next\n",
        "    next: str\n",
        "\n",
        "print(\"âœ… Team State Defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QztkJEaXCRuX",
        "outputId": "0f9dbc46-0585-49b5-f26f-9a1259722542"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Team State Defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Creating the Workers\n",
        "\n",
        "We need two specialized agents.\n",
        "1.  **Researcher:** Has a `web_search` tool.\n",
        "2.  **Coder:** Has a `python_repl` tool.\n",
        "\n",
        "To make this easy, we'll create a helper function `create_agent` that wraps a standard LangChain agent into a graph node."
      ],
      "metadata": {
        "id": "nt6KyzecCkqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "import random\n",
        "\n",
        "# --- 1. Define Mock Tools ---\n",
        "# In a real app, these would be real APIs (Tavily, PythonExec, etc.)\n",
        "\n",
        "@tool\n",
        "def web_search(query: str) -> list[float]:\n",
        "    \"\"\"Search the web for information.\"\"\"\n",
        "    print(f\"    ğŸ” [Tool] Searching for: {query}\")\n",
        "\n",
        "    # Generate 10 random stock prices between 100 and 200\n",
        "    prices = [round(random.uniform(100, 200), 2) for _ in range(10)]\n",
        "    return prices\n",
        "\n",
        "@tool\n",
        "def python_repl(code: str):\n",
        "    \"\"\"Executes python code to generate charts.\"\"\"\n",
        "    print(f\"    ğŸ’» [Tool] Executing Python: {code}\")\n",
        "    return \"Chart generated successfully at /tmp/chart.png\"\n",
        "\n",
        "# --- 2. Helper Function to Build Agents ---\n",
        "def create_agent(llm, tools, system_prompt):\n",
        "    # This creates a standard ReAct agent (Reason -> Act)\n",
        "    # It uses the system prompt to define the persona\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ])\n",
        "    return create_react_agent(llm, tools, prompt=prompt)\n",
        "\n",
        "# --- 3. Create the Specialized Agents ---\n",
        "research_agent = create_agent(\n",
        "    llm,\n",
        "    [web_search],\n",
        "    \"You are a web researcher. You search for accurate data. When you find data, present it clearly and concisely, indicating it is ready for further processing by another agent, such as plotting. Do not attempt to visualize or plot the data yourself.\"\n",
        ")\n",
        "\n",
        "coding_agent = create_agent(\n",
        "    llm,\n",
        "    [python_repl],\n",
        "    \"You are a Data Scientist. You write code to visualize and chart data.\"\n",
        ")\n",
        "\n",
        "# --- 4. Define Node Wrappers ---\n",
        "# These functions bridge the Agent output to the Graph State\n",
        "def research_node(state: AgentState):\n",
        "    result = research_agent.invoke(state)\n",
        "    # We return the last message (the agent's result) to the global state\n",
        "    return {\"messages\": [result[\"messages\"][-1]]}\n",
        "\n",
        "def coding_node(state: AgentState):\n",
        "    result = coding_agent.invoke(state)\n",
        "    return {\"messages\": [result[\"messages\"][-1]]}\n",
        "\n",
        "print(\"âœ… Workers (Researcher & Coder) Created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVfrKbJqCmND",
        "outputId": "9c93525d-1b29-4648-fb58-5c0075978c91"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Workers (Researcher & Coder) Created.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-783237345.py:32: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  return create_react_agent(llm, tools, prompt=prompt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: The Supervisor (The Router)\n",
        "\n",
        "This is the brain of the operation.\n",
        "The Supervisor is an LLM Chain that:\n",
        "1.  Reads the conversation history.\n",
        "2.  Decides which worker should act next.\n",
        "3.  Or decides to `FINISH` if the user's request is satisfied.\n",
        "\n",
        "We use **OpenAI Function Calling** to force the LLM to output a structured decision (`next`)."
      ],
      "metadata": {
        "id": "RfovThqaDrSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "\n",
        "members = [\"Researcher\", \"Coder\"]\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    \" following workers: {members}. Given the user request, determine which worker\"\n",
        "    \" should act next. Each worker will perform a task and respond with their results.\"\n",
        "    \" Based on the conversation history and the user's initial request, decide whether:\"\n",
        "    \" 1. Another worker needs to perform a task (e.g., Researcher to gather data, Coder to plot).\"\n",
        "    \" 2. The original request has been fully satisfied and the process should FINISH.\"\n",
        "    \" Prioritize completing the original request, like finding data and then plotting it.\"\n",
        "    \" If the Researcher has found data, and plotting is required, delegate to the Coder.\"\n",
        "    \" If all aspects of the request are handled, then FINISH.\"\n",
        ")\n",
        "\n",
        "# --- Define the Routing Schema ---\n",
        "# This tells the LLM: \"You MUST pick one of these options.\"\n",
        "options = [\"FINISH\"] + members\n",
        "function_def = {\n",
        "    \"name\": \"route\",\n",
        "    \"description\": \"Select the next role.\",\n",
        "    \"parameters\": {\n",
        "        \"title\": \"routeSchema\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"next\": {\n",
        "                \"title\": \"Next\",\n",
        "                \"anyOf\": [{\"enum\": options}],\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"next\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# --- Build the Chain ---\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    (\"system\", \"Given the conversation above, who should act next? Or should we FINISH? Select one: {options}\"),\n",
        "]).partial(options=str(options), members=\", \".join(members))\n",
        "\n",
        "supervisor_chain = (\n",
        "    prompt\n",
        "    | llm.bind(functions=[function_def], function_call={\"name\": \"route\"})\n",
        "    | JsonOutputFunctionsParser()\n",
        ")\n",
        "\n",
        "print(\"âœ… Supervisor Chain Created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbwMv9DnDtcx",
        "outputId": "684258fa-8bfa-440f-802f-02d382c18211"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Supervisor Chain Created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: The Graph Architecture\n",
        "\n",
        "We wire the nodes in a **Star Topology**:\n",
        "1.  **Supervisor** is the center.\n",
        "2.  **Workers** (Researcher, Coder) are the spokes.\n",
        "3.  After a Worker finishes, they **always** report back to the Supervisor."
      ],
      "metadata": {
        "id": "wFawpR-_EPWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# 1. Add Nodes\n",
        "workflow.add_node(\"Supervisor\", supervisor_chain)\n",
        "workflow.add_node(\"Researcher\", research_node)\n",
        "workflow.add_node(\"Coder\", coding_node)\n",
        "\n",
        "# 2. Add Edges\n",
        "# Start at Supervisor\n",
        "workflow.add_edge(START, \"Supervisor\")\n",
        "\n",
        "# Workers always go back to Supervisor\n",
        "workflow.add_edge(\"Researcher\", \"Supervisor\")\n",
        "workflow.add_edge(\"Coder\", \"Supervisor\")\n",
        "\n",
        "# 3. Conditional Logic (The Routing)\n",
        "# Based on the 'next' field from the Supervisor, where do we go?\n",
        "workflow.add_conditional_edges(\n",
        "    \"Supervisor\",\n",
        "    lambda x: x[\"next\"],\n",
        "    {\n",
        "        \"Researcher\": \"Researcher\",\n",
        "        \"Coder\": \"Coder\",\n",
        "        \"FINISH\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# 4. Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"âœ… Multi-Agent Graph Compiled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt4deSU6D5M3",
        "outputId": "43f0655d-ee58-4d4d-f40c-adb27146d651"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Multi-Agent Graph Compiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Execution\n",
        "\n",
        "Let's test the system with a multi-step request:\n",
        "**\"Research the stock price of Apple and then plot a chart.\"**\n",
        "\n",
        "Watch the output carefully:\n",
        "1.  Supervisor -> Researcher (to get data)\n",
        "2.  Researcher -> Supervisor (returns data)\n",
        "3.  Supervisor -> Coder (to plot data)\n",
        "4.  Coder -> Supervisor (returns success)\n",
        "5.  Supervisor -> FINISH"
      ],
      "metadata": {
        "id": "U0QeLAkCEbDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- ğŸš€ Starting Multi-Agent Workflow ---\")\n",
        "initial_state = {\"messages\": [(\"user\", \"Research the stock price of Apple and then plot a chart.\")]}\n",
        "\n",
        "# We stream the output to see the steps\n",
        "for s in app.stream(initial_state):\n",
        "    if \"__end__\" not in s:\n",
        "        # Print the node name and the output\n",
        "        node_name = list(s.keys())[0]\n",
        "        print(f\"\\nğŸ“ Node: {node_name}\")\n",
        "\n",
        "        if node_name == \"Supervisor\":\n",
        "            print(f\"   Decision: {s[node_name]['next']}\")\n",
        "        else:\n",
        "            # Print the last message from the worker\n",
        "            last_msg = s[node_name]['messages'][0]\n",
        "            # Handle both ToolMessages and AIMessages\n",
        "            content = getattr(last_msg, 'content', str(last_msg))\n",
        "            print(f\"   Output: {content[:100]}...\") # Truncate for readability"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oXdbDHMAEeEA",
        "outputId": "ab6cfa01-872c-4b58-8bb2-6f14502c7422"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ğŸš€ Starting Multi-Agent Workflow ---\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    ğŸ” [Tool] Searching for: Apple stock price\n",
            "\n",
            "ğŸ“ Node: Researcher\n",
            "   Output: Here is a simple line chart representing the recent stock prices of Apple:\n",
            "\n",
            "```plaintext\n",
            "Price ($)\n",
            " ...\n",
            "\n",
            "ğŸ“ Node: Supervisor\n",
            "   Decision: Researcher\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "GraphRecursionError",
          "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1355621429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# We stream the output to see the steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"__end__\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Print the node name and the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAPH_RECURSION_LIMIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                 )\n\u001b[0;32m-> 2671\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mGraphRecursionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m             \u001b[0;31m# set final channel values as run output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "You have built a **Hierarchical Multi-Agent System**.\n",
        "\n",
        "**Why is this better than a single agent?**\n",
        "1.  **Separation of Concerns:** The Coder doesn't need to know how to Search. The Researcher doesn't need to know Python.\n",
        "2.  **Context Management:** The Supervisor keeps the team focused.\n",
        "3.  **Modularity:** You can easily add a \"Writer\" or \"Reviewer\" agent just by adding a node and updating the `members` list.\n",
        "\n",
        "This architecture is the foundation for building complex, enterprise-grade AI assistants."
      ],
      "metadata": {
        "id": "jQvzi6M7JFYO"
      }
    }
  ]
}