{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-jamwal/blog-building-agentic-architectures/blob/main/langgraph/Part_2_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baPfjm2KFDha"
      },
      "source": [
        "# Building Agents in Python and n8n in 2026\n",
        "Companion to https://rajajamwal.substack.com/p/building-agents-in-python-and-n8n\n",
        "\n",
        "Subscribe to my blog, https://rajajamwal.substack.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QSj6HY6FJDl"
      },
      "source": [
        "## Orchestrating Intelligence: Building a Multi-Agent Supervisor\n",
        "\n",
        "Welcome to Part 2 of the LangGraph implementation guide.\n",
        "\n",
        "In Part 1, we built a single \"Super-Node\" agent.\n",
        "In Part 2, we are scaling up. We will build a **Multi-Agent System**.\n",
        "\n",
        "### The Architecture: Hub & Spoke\n",
        "Instead of one agent trying to do everything, we will create:\n",
        "1.  **The Supervisor (Hub):** A \"Project Manager\" LLM that plans and delegates.\n",
        "2.  **The Workers (Spokes):** Specialized agents (Researcher, Coder) that execute tasks.\n",
        "\n",
        "### The Stack\n",
        "*   **LangGraph:** For the cyclic graph topology.\n",
        "*   **LangChain:** For agent definitions.\n",
        "*   **OpenAI:** GPT-4o (The Supervisor needs a smart model).\n",
        "\n",
        "### The Goal\n",
        "Handle a complex request: *\"Research the current stock price of Apple and plot a chart.\"*\n",
        "*   The **Researcher** will find the data.\n",
        "*   The **Coder** will generate the plot code.\n",
        "*   The **Supervisor** will manage the handoffs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWcM_mTnG5X_",
        "outputId": "37adcf0d-e6d2-4e0a-b2c1-bbf862120898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/105.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/84.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/489.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Environment Setup Complete.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies\n",
        "!pip install -qU langgraph langchain langchain-openai langchain-core\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# @title 2. Setup API Key\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Initialize the Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# We use GPT-4o because routing requires high reasoning capabilities\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "print(\"‚úÖ Environment Setup Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gERzhHBYCQJt"
      },
      "source": [
        "## Step 1: Defining the Team State\n",
        "\n",
        "Our state needs to track two things:\n",
        "1.  `messages`: The global conversation history (so everyone sees what has been done).\n",
        "2.  `next`: A string indicating **who acts next** (e.g., \"Researcher\", \"Coder\", or \"FINISH\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QztkJEaXCRuX",
        "outputId": "eb29d49b-e497-4ce4-f454-6bc24eac2c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Team State Defined.\n"
          ]
        }
      ],
      "source": [
        "from typing import Annotated, TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    next: str\n",
        "    data_ready: bool\n",
        "    chart_done: bool\n",
        "\n",
        "print(\"‚úÖ Team State Defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt6KyzecCkqK"
      },
      "source": [
        "## Step 2: Creating the Workers\n",
        "\n",
        "We need two specialized agents.\n",
        "1.  **Researcher:** Has a `web_search` tool.\n",
        "2.  **Coder:** Has a `python_repl` tool.\n",
        "\n",
        "To make this easy, we'll create a helper function `create_agent` that wraps a standard LangChain agent into a graph node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVfrKbJqCmND",
        "outputId": "8d29109b-cb8f-44fa-e307-34d5174d9e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Workers (Researcher & Coder) Created.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3982199780.py:32: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  return create_react_agent(llm, tools, prompt=prompt)\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "import random\n",
        "\n",
        "# --- 1. Define Mock Tools ---\n",
        "# In a real app, these would be real APIs (Tavily, PythonExec, etc.)\n",
        "\n",
        "@tool\n",
        "def web_search(query: str) -> list[float]:\n",
        "    \"\"\"Search the web for information.\"\"\"\n",
        "    print(f\"    üîé [Tool] Searching for: {query}\")\n",
        "\n",
        "    # Generate 10 random stock prices between 100 and 200\n",
        "    prices = [round(random.uniform(100, 200), 2) for _ in range(10)]\n",
        "    return prices\n",
        "\n",
        "@tool\n",
        "def python_repl(code: str):\n",
        "    \"\"\"Executes python code to generate charts.\"\"\"\n",
        "    print(f\"    üíª [Tool] Executing Python: {code}\")\n",
        "    return \"Chart generated successfully at /tmp/chart.png\"\n",
        "\n",
        "# --- 2. Helper Function to Build Agents ---\n",
        "def create_agent(llm, tools, system_prompt):\n",
        "    # This creates a standard ReAct agent (Reason -> Act)\n",
        "    # It uses the system prompt to define the persona\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ])\n",
        "    return create_react_agent(llm, tools, prompt=prompt)\n",
        "\n",
        "# --- 3. Create the Specialized Agents ---\n",
        "research_agent = create_agent(\n",
        "    llm,\n",
        "    [web_search],\n",
        "    \"\"\"\n",
        "You are a web researcher.\n",
        "\n",
        "TASK:\n",
        "- Find the stock price data for Apple.\n",
        "\n",
        "RULES:\n",
        "- DO NOT plot or visualize anything.\n",
        "- DO NOT describe charts.\n",
        "- Return ONLY the raw price data.\n",
        "- End your message with the exact phrase: DATA_READY\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "coding_agent = create_agent(\n",
        "    llm,\n",
        "    [python_repl],\n",
        "    \"\"\"\n",
        "You are a data scientist.\n",
        "\n",
        "TASK:\n",
        "- Use the provided stock price data\n",
        "- Generate Python code to plot a chart\n",
        "\n",
        "RULES:\n",
        "- Do not search for data\n",
        "- Only generate plotting code\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# --- 4. Define Node Wrappers ---\n",
        "# These functions bridge the Agent output to the Graph State\n",
        "def research_node(state: AgentState):\n",
        "    result = research_agent.invoke(state)\n",
        "    return {\n",
        "        \"messages\": [result[\"messages\"][-1]],\n",
        "        \"data_ready\": True\n",
        "    }\n",
        "\n",
        "def coding_node(state: AgentState):\n",
        "    result = coding_agent.invoke(state)\n",
        "    return {\n",
        "        \"messages\": [result[\"messages\"][-1]],\n",
        "        \"chart_done\": True\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Workers (Researcher & Coder) Created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfovThqaDrSE"
      },
      "source": [
        "## Step 3: The Supervisor (The Router)\n",
        "\n",
        "This is the brain of the operation.\n",
        "The Supervisor is an LLM Chain that:\n",
        "1.  Reads the conversation history.\n",
        "2.  Decides which worker should act next.\n",
        "3.  Or decides to `FINISH` if the user's request is satisfied.\n",
        "\n",
        "We use **OpenAI Function Calling** to force the LLM to output a structured decision (`next`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbwMv9DnDtcx",
        "outputId": "dce8b7c3-e9b6-499f-d7fe-38038d15b5da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Supervisor Chain Created.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "\n",
        "members = [\"Researcher\", \"Coder\"]\n",
        "\n",
        "system_prompt = f\"\"\"\n",
        "You are a supervisor managing the following workers:\n",
        "{\", \".join(members)}\n",
        "\n",
        "ROUTING RULES:\n",
        "- If data_ready is false ‚Üí send to Researcher\n",
        "- If data_ready is true and chart_done is false ‚Üí send to Coder\n",
        "- If data_ready and chart_done are both true ‚Üí FINISH\n",
        "\n",
        "Do NOT repeat tasks unnecessarily.\n",
        "\"\"\"\n",
        "\n",
        "# --- Define the Routing Schema ---\n",
        "# This tells the LLM: \"You MUST pick one of these options.\"\n",
        "options = [\"FINISH\"] + members\n",
        "function_def = {\n",
        "    \"name\": \"route\",\n",
        "    \"description\": \"Select the next role.\",\n",
        "    \"parameters\": {\n",
        "        \"title\": \"routeSchema\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"next\": {\n",
        "                \"title\": \"Next\",\n",
        "                \"anyOf\": [{\"enum\": options}],\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"next\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# --- Build the Chain ---\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    (\"system\", \"Given the conversation above, who should act next? Or should we FINISH? Select one: {options}\"),\n",
        "]).partial(options=str(options), members=\", \".join(members))\n",
        "\n",
        "supervisor_chain = (\n",
        "    prompt\n",
        "    | llm.bind(functions=[function_def], function_call={\"name\": \"route\"})\n",
        "    | JsonOutputFunctionsParser()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Supervisor Chain Created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFawpR-_EPWK"
      },
      "source": [
        "## Step 4: The Graph Architecture\n",
        "\n",
        "We wire the nodes in a **Star Topology**:\n",
        "1.  **Supervisor** is the center.\n",
        "2.  **Workers** (Researcher, Coder) are the spokes.\n",
        "3.  After a Worker finishes, they **always** report back to the Supervisor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt4deSU6D5M3",
        "outputId": "893dff54-fec5-403c-86b4-3a23ef99e105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Multi-Agent Graph Compiled.\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# 1. Add Nodes\n",
        "workflow.add_node(\"Supervisor\", supervisor_chain)\n",
        "workflow.add_node(\"Researcher\", research_node)\n",
        "workflow.add_node(\"Coder\", coding_node)\n",
        "\n",
        "# 2. Add Edges\n",
        "# Start at Supervisor\n",
        "workflow.add_edge(START, \"Supervisor\")\n",
        "\n",
        "# Workers always go back to Supervisor\n",
        "workflow.add_edge(\"Researcher\", \"Supervisor\")\n",
        "workflow.add_edge(\"Coder\", \"Supervisor\")\n",
        "\n",
        "def supervisor_router(state: AgentState):\n",
        "    if not state.get(\"data_ready\", False):\n",
        "        return \"Researcher\"\n",
        "    if state.get(\"data_ready\") and not state.get(\"chart_done\", False):\n",
        "        return \"Coder\"\n",
        "    return \"FINISH\"\n",
        "\n",
        "# 3. Conditional Logic (The Routing)\n",
        "# Based on the 'next' field from the Supervisor, where do we go?\n",
        "workflow.add_conditional_edges(\n",
        "    \"Supervisor\",\n",
        "    supervisor_router,\n",
        "    {\n",
        "        \"Researcher\": \"Researcher\",\n",
        "        \"Coder\": \"Coder\",\n",
        "        \"FINISH\": END,\n",
        "    }\n",
        ")\n",
        "\n",
        "# 4. Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"‚úÖ Multi-Agent Graph Compiled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0QeLAkCEbDz"
      },
      "source": [
        "## Step 5: Execution\n",
        "\n",
        "Let's test the system with a multi-step request:\n",
        "**\"Research the stock price of Apple and then plot a chart.\"**\n",
        "\n",
        "Watch the output carefully:\n",
        "1.  Supervisor -> Researcher (to get data)\n",
        "2.  Researcher -> Supervisor (returns data)\n",
        "3.  Supervisor -> Coder (to plot data)\n",
        "4.  Coder -> Supervisor (returns success)\n",
        "5.  Supervisor -> FINISH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXdbDHMAEeEA",
        "outputId": "d7ac6f7b-c25c-4911-fb64-301b89dd1b7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- üöÄ Starting Multi-Agent Workflow ---\n",
            "\n",
            "üìç Node: Supervisor\n",
            "   Decision: Researcher\n",
            "    üîé [Tool] Searching for: Apple stock price data\n",
            "\n",
            "üìç Node: Researcher\n",
            "   Output: [192.74, 170.51, 158.97, 125.61, 130.08, 103.57, 153.89, 167.23, 164.48, 190.01]\n",
            "DATA_READY...\n",
            "\n",
            "üìç Node: Supervisor\n",
            "   Decision: Coder\n",
            "    üíª [Tool] Executing Python: import matplotlib.pyplot as plt\n",
            "\n",
            "# Apple stock prices\n",
            "dates = ['2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01', '2023-05-01', '2023-06-01', '2023-07-01', '2023-08-01', '2023-09-01', '2023-10-01']\n",
            "prices = [192.74, 170.51, 158.97, 125.61, 130.08, 103.57, 153.89, 167.23, 164.48, 190.01]\n",
            "\n",
            "plt.figure(figsize=(10, 5))\n",
            "plt.plot(dates, prices, marker='o', linestyle='-', color='b')\n",
            "plt.title('Apple Stock Prices (2023)')\n",
            "plt.xlabel('Date')\n",
            "plt.ylabel('Price (USD)')\n",
            "plt.xticks(rotation=45)\n",
            "plt.grid(True)\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "\n",
            "üìç Node: Coder\n",
            "   Output: Here is the chart of Apple's stock prices for 2023:\n",
            "\n",
            "![Apple Stock Prices (2023)](/tmp/chart.png)...\n",
            "\n",
            "üìç Node: Supervisor\n",
            "   Decision: Researcher\n"
          ]
        }
      ],
      "source": [
        "print(\"--- üöÄ Starting Multi-Agent Workflow ---\")\n",
        "initial_state = {\n",
        "    \"messages\": [(\"user\", \"Research the stock price of Apple and then plot a chart.\")],\n",
        "    \"data_ready\": False,\n",
        "    \"chart_done\": False\n",
        "}\n",
        "\n",
        "# We stream the output to see the steps\n",
        "for s in app.stream(initial_state):\n",
        "    if \"__end__\" not in s:\n",
        "        # Print the node name and the output\n",
        "        node_name = list(s.keys())[0]\n",
        "        print(f\"\\nüìç Node: {node_name}\")\n",
        "\n",
        "        if node_name == \"Supervisor\":\n",
        "            print(f\"   Decision: {s[node_name]['next']}\")\n",
        "        else:\n",
        "            # Print the last message from the worker\n",
        "            last_msg = s[node_name]['messages'][0]\n",
        "            # Handle both ToolMessages and AIMessages\n",
        "            content = getattr(last_msg, 'content', str(last_msg))\n",
        "            print(f\"   Output: {content[:100]}...\") # Truncate for readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQvzi6M7JFYO"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You have built a **Hierarchical Multi-Agent System**.\n",
        "\n",
        "**Why is this better than a single agent?**\n",
        "1.  **Separation of Concerns:** The Coder doesn't need to know how to Search. The Researcher doesn't need to know Python.\n",
        "2.  **Context Management:** The Supervisor keeps the team focused.\n",
        "3.  **Modularity:** You can easily add a \"Writer\" or \"Reviewer\" agent just by adding a node and updating the `members` list.\n",
        "\n",
        "This architecture is the foundation for building complex, enterprise-grade AI assistants."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMyKgnfdYcXL7YRgs8nqmg8",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
